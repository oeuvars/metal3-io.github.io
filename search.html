<!doctype html>
<html class="no-js" lang="en">

<head>
    <script id="dpal" src="//www.redhat.com/ma/dpal.js" type="text/javascript"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <meta name="theme-color" content="#008585">
    
    <title>Metal¬≥ - Metal Kubed</title>
    <!-- # Opengraph protocol properties: https://ogp.me/ -->
    <meta name="author" content="The Metal¬≥ - Metal Kubed website team, " >
    
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">
    <meta name="keywords" content="hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic, openshift, kubernetes, openstack, operator, summit, kubecon, shiftdev, metal3-dev-env, documentation, development, talk, conference, meetup, cluster-api, provider, raw-image, image-streaming, ipam, ip-address-manager, pivoting, move, " >
    <meta property="og:title" content="Metal¬≥ - Metal Kubed">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://metal3.io/search.html" >
    <meta property="og:image" content="https://metal3.io/assets/images/metal3logo.png">
    <meta property="og:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes." >
    <meta property="og:site_name" content="Metal¬≥ - Metal Kubed" >
    <meta property="og:article:author" content="The Metal¬≥ - Metal Kubed website team" >
    <meta property="og:article:published_time" content="2023-04-14 19:17:36 -0500" >
    <meta name="twitter:title" content="Metal¬≥ - Metal Kubed">
    <meta name="twitter:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">

    <link type="application/atom+xml" rel="alternate" href="https://metal3.io/feed.xml" title="Metal¬≥ - Metal Kubed" />
    <meta name="google-site-verification" content="HCdbGknTOCTKQVt7m-VxTG4BEYXxSqm-sDb-iklqrB0" />
  <link href="https://fonts.googleapis.com/css?family=Nunito:200,400&display=swap" rel="stylesheet">
  <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
  <!-- Photoswipe.com gallery-->

  <!-- Core CSS file -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">

  <!-- Skin CSS file (styling of UI - buttons, caption, etc.)
      In the folder of skin CSS file there are also:
      - .png and .svg icons sprite,
      - preloader.gif (for browsers that do not support CSS animations) -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">
</head>
<body>
    <!--[if IE]>
      <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
    <![endif]-->

<div class="mk-wrapper">
    <section class="mk-masthead mk-masthead--sub">
<header class="mk-main-header">
    <a href="/" class="mk-main-header__brand">
        <svg version="1.1" viewBox="0 0 557 540" xmlns="http://www.w3.org/2000/svg">
          <g fill="none" fill-rule="evenodd">
            <g transform="translate(-1)" fill-rule="nonzero">
            <path d="m181.91 539.68h-0.7c-0.76 0-1.44-0.11-2-0.17h-0.14l-1.62-0.2c-15.204-1.867-29.364-8.7129-40.27-19.47l-1.07-1.06-49.46-61.26-73.34-90.59-0.5-0.72c-2.8927-4.0899-5.2989-8.503-7.17-13.15-1.0257-2.532-1.8875-5.1274-2.58-7.77v-0.11c-0.22-0.85-0.43-1.69-0.62-2.56v-0.14c-0.8042-3.5966-1.2861-7.2578-1.44-10.94v-0.47-0.48c-0.067687-4.136 0.26722-8.2688 1-12.34l0.11-0.61 14.51-63.64 28.72-126c4.0017-17.442 15.802-32.074 32-39.68l178.2-85.93 3.34-0.67c5.6926-1.1418 11.484-1.7201 17.29-1.7201h2.83 0.57c8.4518-0.016309 16.808 1.7879 24.5 5.2901l0.47 0.22 175.82 84.2 0.48 0.25c7.1101 3.7526 13.491 8.7481 18.84 14.75 2.7886 3.1018 5.2639 6.4715 7.39 10.06l0.17 0.29c2.1776 3.7964 3.9314 7.8205 5.23 12l0.3 1 44.23 190.25 0.17 1.42c2.0399 16.443-2.1677 33.052-11.79 46.54l-0.48 0.68-121.46 150.24c-7.2792 9.604-17.475 16.59-29.06 19.91-0.93 0.27-1.87 0.52-2.81 0.75l-0.3 0.07c-5.0328 1.1701-10.183 1.76-15.35 1.76h-194.01z" fill="#fff"/>
            <path d="m492 131.65c-0.75221-2.3458-1.7582-4.6025-3-6.73-1.2507-2.1148-2.7114-4.0982-4.36-5.92-3.3032-3.7145-7.2456-6.8067-11.64-9.13l-179.82-86c-4.3569-1.9816-9.0938-2.9883-13.88-2.95h-0.77c-4.9428-0.19294-9.8909 0.20318-14.74 1.18l-179.72 86.6c-8.9642 4.1124-15.498 12.17-17.67 21.79l-3.69 16.16 216.29 117.67 0.34-0.18 217.22-112.72-4.56-19.77z" fill="#00E0C1"/>
            <path d="m279 264.32l-216.29-117.67-25.77 113.1-14.73 64.63c-0.44744 2.4671-0.64178 4.9734-0.58 7.48v0.29c0.072639 2.1493 0.33702 4.2878 0.79 6.39 0.12 0.56 0.26 1.1 0.4 1.65 0.41228 1.5719 0.92671 3.1151 1.54 4.62 1.1152 2.7748 2.5517 5.4095 4.28 7.85l23.69 29.27 51 63 49.67 61.55c6.7982 6.7311 15.643 11.009 25.14 12.16 0.67 0 1.31 0.18 2 0.21h99.17v-254.34l-0.31-0.19z" fill="#00EEC4"/>
            <path d="m536.75 324.38l-40.19-173-217.23 112.76v254.71h98.82c3.2616 0.017 6.5139-0.34884 9.69-1.0906 0.62-0.15 1.23-0.31 1.84-0.49 6.2438-1.7629 11.72-5.5604 15.56-10.79l66.09-81.75 31.31-38.73 26.94-33.33c5.8432-8.2018 8.4013-18.295 7.17-28.29z" fill="#00D1BD"/>
            <path d="m120.94 369l137 75.89c1.3702 0.76284 3.0421 0.74251 4.3933-0.05344s2.1796-2.2483 2.1767-3.8166v-161.02c0-5.718-3.1489-10.971-8.19-13.67l-1.64-0.87-134.68-71.99c-0.8041-0.43178-1.7757-0.41032-2.56 0.056543-0.78426 0.46687-1.2663 1.3108-1.27 2.2235l-0.94 163.17c0.02 3.63 2.77 8.44 5.71 10.08z" fill="#fff"/>
            <path d="m282.61 103.85c-4.0372-0.033083-8.0333 0.81323-11.71 2.481l-134.2 60.47c-0.91184 0.40637-1.512 1.2973-1.5476 2.295-0.032512 0.99771 0.50554 1.9274 1.3876 2.395l135.72 72.51c0.15 0.09 0.31 0.16 0.47 0.24l0.59 0.29 0.26 0.11 0.8 0.34h0.09c4.9879 1.8704 10.539 1.5061 15.24-1l139.14-73.94c1.1079-0.5822 1.7814-1.7504 1.7328-3.0009-0.054039-1.2505-0.82096-2.3596-1.9728-2.8491l-135.06-58.05c-3.4545-1.4945-7.1761-2.2735-10.94-2.291z" fill="#fff"/>
            <path d="m442.82 192.61c-1.08-0.49333-2.4133-0.29667-4 0.59l-24.52 13.54c-3.6117 1.9922-6.3845 5.2194-7.81 9.09l-37.49 87.55-37.31-46.2c-1.59-2.29-4.2-2.45-7.81-0.45l-24.51 13.54c-1.6358 0.9266-3.0116 2.2508-4 3.85-1.0039 1.4454-1.5667 3.151-1.62 4.91v166.83c0 1.59 0.55 2.59 1.63 3s2.42 0.19 4-0.69l27.34-15.1c1.6143-0.90735 2.9864-2.1902 4-3.74 1.0178-1.3976 1.5863-3.0717 1.63-4.8v-105l23.21 30.12c2.1667 2.1267 4.6967 2.3933 7.59 0.8l11.67-6.45c3.18-1.7467 5.71-4.8067 7.59-9.18l23.43-55.9 0.25 97.1 0.17 8.31c-0.10795 1.217 0.57186 2.3675 1.69 2.86 1.2747 0.44018 2.6836 0.23517 3.78-0.55l27.27-15.83c1.5869-0.9387 2.9245-2.2455 3.9-3.81 0.9881-1.4207 1.5184-3.1095 1.5212-4.84v-166.43c0.028815-1.6-0.51118-2.64-1.6012-3.12z" fill="#fff"/>
            </g>
          </g>
        </svg>
      </a>
      <nav role="navigation" class="mk-main-header__nav-wrapper">
        <button class="mk-main-header__toggle" id="toggle" aria-controls="main_nav" aria-expanded="false" aria-label="navigation toggle" >
          <svg version="1.1" viewBox="0 0 512 448" xmlns="http://www.w3.org/2000/svg">
          <g>
          <path d="m296 0h192c13.255 0 24 10.745 24 24v160c0 13.255-10.745 24-24 24h-192c-13.255 0-24-10.745-24-24v-160c0-13.255 10.745-24 24-24zm-80 0h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24zm-216 264v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24zm296 184h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24z"/>
          </g>
          </svg>
          <span class="mk-main-header__toggle-text">menu</span>
        </button>
        <ul id="main_nav" class="mk-main-nav">
          <li ><a class="mk-main-nav__item" href="/blog/index.html">Blog</a></li>
          <li ><a class="mk-main-nav__item" href="/community-resources.html">Community Resources</a></li>
          <li ><a class="mk-main-nav__item" href="/documentation.html">Documentation</a></li>
          <li ><a class="mk-main-nav__item" href="/try-it.html">Try It!</a></li>
          <li  class="active" >
            <form action="/search.html" method="get" autocomplete="off">
              <div class="autocomplete" style="width:150px;">
                <input type="text" id="search-input" class="docs-search--input" placeholder="search term" name="query">
              </div>
              <input id="search-button" type="submit" value="üîç" disabled='true'>
            </form>
          </li>
      </li>


        </ul>
      </nav>
  </header>
  
<script>
function autocomplete(inp, arr) {
  /*the autocomplete function takes two arguments,
  the text field element and an array of possible autocompleted values:*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener("input", function(e) {
      var a, b, i, val = this.value;
      /*close any already open lists of autocompleted values*/
      closeAllLists();
      if (!val) { return false;}
      currentFocus = -1;
      /*create a DIV element that will contain the items (values):*/
      a = document.createElement("DIV");
      a.setAttribute("id", this.id + "autocomplete-list");
      a.setAttribute("class", "autocomplete-items");
      /*append the DIV element as a child of the autocomplete container:*/
      this.parentNode.appendChild(a);
      /*for each item in the array...*/
      for (i = 0; i < arr.length; i++) {
        /*check if the item starts with the same letters as the text field value:*/
        if (arr[i].substr(0, val.length).toUpperCase() == val.toUpperCase()) {
          /*create a DIV element for each matching element:*/
          b = document.createElement("DIV");
          /*make the matching letters bold:*/
          b.innerHTML = "<strong>" + arr[i].substr(0, val.length) + "</strong>";
          b.innerHTML += arr[i].substr(val.length);
          /*insert a input field that will hold the current array item's value:*/
          b.innerHTML += "<input type='hidden' value='" + arr[i] + "'>";
          /*execute a function when someone clicks on the item value (DIV element):*/
              b.addEventListener("click", function(e) {
              /*insert the value for the autocomplete text field:*/
              inp.value = this.getElementsByTagName("input")[0].value;
              /*close the list of autocompleted values,
              (or any other open lists of autocompleted values:*/
              closeAllLists();
          });
          a.appendChild(b);
        }
      }
  });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener("keydown", function(e) {
      document.getElementById("search-button").disabled= undefined;
      var x = document.getElementById(this.id + "autocomplete-list");
      if (x) x = x.getElementsByTagName("div");
      if (e.keyCode == 40) {
        /*If the arrow DOWN key is pressed,
        increase the currentFocus variable:*/
        currentFocus++;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 38) { //up
        /*If the arrow UP key is pressed,
        decrease the currentFocus variable:*/
        currentFocus--;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 13) {
        /*If the ENTER key is pressed, prevent the form from being submitted,*/
        if (currentFocus > -1) {
          /*and simulate a click on the "active" item:*/
          if (x) {
            x[currentFocus].click();
            e.preventDefault();
          }
        }
        if (document.getElementById("search-input").value == "") {
          e.preventDefault();
        }
      }
  });
  function addActive(x) {
    /*a function to classify an item as "active":*/
    if (!x) return false;
    /*start by removing the "active" class on all items:*/
    removeActive(x);
    if (currentFocus >= x.length) currentFocus = 0;
    if (currentFocus < 0) currentFocus = (x.length - 1);
    /*add class "autocomplete-active":*/
    x[currentFocus].classList.add("autocomplete-active");
  }
  function removeActive(x) {
    /*a function to remove the "active" class from all autocomplete items:*/
    for (var i = 0; i < x.length; i++) {
      x[i].classList.remove("autocomplete-active");
    }
  }
  function closeAllLists(elmnt) {
    /*close all autocomplete lists in the document,
    except the one passed as an argument:*/
    var x = document.getElementsByClassName("autocomplete-items");
    for (var i = 0; i < x.length; i++) {
      if (elmnt != x[i] && elmnt != inp) {
      x[i].parentNode.removeChild(x[i]);
    }
  }
}
/*execute a function when someone clicks in the document:*/
document.addEventListener("click", function (e) {
    closeAllLists(e.target);
});
}
</script>
<script>
var mykeywords = ["hybrid", "cloud", "metal3", "baremetal", "stack", "edge", "openstack", "ironic", "openshift", "kubernetes", "OpenStack", "operator", "summit", "kubecon", "shiftdev", "metal3-dev-env", "documentation", "development", "talk", "conference", "meetup", "cluster API", "provider", "raw image", "image streaming", "IPAM", "ip address manager", "Pivoting", "Move", ]
autocomplete(document.getElementById("search-input"), mykeywords);
</script>
<script src="/assets/js/clipboard.min.js"></script>
<!-- Photoswipe -->
<!-- Core JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<!-- UI JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="/assets/js/lunr.min.js"></script>

<div class="mk-masthead__content--sub">
        <h1 class="mk-masthead__content--sub__title">Search results</h1>
</div>
</section>
<main class="mk-main mk-blog">
            <article class="mk-main__section mk-main__content mk-main__section__content">
    <div class="container post">
      <h1 class="page-title"></h1>
      <article class="post-content">
        <div id="lunrsearchresults">
            <ul></ul>
        </div>
      </article>
    </div>

</article>
<nav class="mk-pagination">
        
        
</nav>

    </main>



<script>

var documents = [{
    "id": 0,
    "url": "/blog/2022/07/08/One_cluster_multiple_providers.html",
    "title": "One cluster - multiple providers",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, hybrid, edge",
    "body": "Running on bare metal has both benefits and drawbacks. You can get the best performance possible out of the hardware, but it can also be quite expensive and maybe not necessary for all workloads. Perhaps a hybrid cluster could give you the best of both?Raw power for the workload that needs it, and cheap virtualized commodity for the rest. This blog post will show how to set up a cluster like this using the Cluster API backed by the Metal3 and BYOH providers. The problem: Imagine that you have some bare metal servers that you want to use for some specific workload. Maybe the workload benefits from the specific hardware or there are some requirements that make it necessary to run it there. The rest of the organization already uses Kubernetes and the cluster API everywhere so of course you want the same for this as well. Perfect, grab Metal¬≥ and start working! But hold on, this would mean that you use some of the servers for running the Kubernetes control plane and possibly all the cluster API controllers. If there are enough servers this is probably not an issue, but do you really want to ‚Äúwaste‚Äù these servers on such generic workloads that could be running anywhere?This can become especially painful if you need multiple control plane nodes. Each server is probably powerful enough to run all the control planes and controllers, but it would be a single point of failure‚Ä¶ What if there was a way to use a different cluster API infrastructure provider for some nodes?For example, use the Openstack infrastructure provider for the control plane and Metal¬≥ for the workers. Let‚Äôs do an experiment! Setting up the experiment environment: This blog post will use the Bring your own host (BYOH) provider together with Metal¬≥ as a proof of concept to show what is currently possible. The BYOH provider was chosen as the second provider for two reasons:  Due to its design (you provision the host yourself), it is very easy to adapt it to the test (e. g. use a VM in the same network that the metal3-dev-env uses).  It is one of the providers that is known to work when combining multiple providers for a single cluster. We will be using the metal3-dev-env on Ubuntu as a starting point for this experiment. Note that it makes substantial changes to the machine where it is running, so you may want to use a dedicated lab machine instead of your laptop for this. If you have not done so already, clone it and run make. This should give you a management cluster with the Metal¬≥ provider installed and two BareMetalHosts ready for provisioning. The next step is to add the BYOH provider and a ByoHost. clusterctl init --infrastructure byohFor the ByoHost we will use Vagrant. You can install it with sudo apt install vagrant. Then copy the Vagrantfile below to a new folder and run vagrant up. # -*- mode: ruby -*-hosts = {   control-plane1  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 10 },  #  control-plane2  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 11 },  #  control-plane3  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 12 },}Vagrant. configure( 2 ) do |config|  # Choose which box you want below  config. vm. box =  generic/ubuntu2004   config. vm. synced_folder  .  ,  /vagrant , disabled: true  config. vm. provider :libvirt do |libvirt|   # QEMU system connection is required for private network configuration   libvirt. qemu_use_session = false  end  # Loop over all machine names  hosts. each_key do |host|    config. vm. define host, primary: host == hosts. keys. first do |node|      node. vm. hostname = host      node. vm. network :private_network, ip: hosts[host][ ip ],       libvirt__forward_mode:  route       node. vm. provider :libvirt do |lv|        lv. memory = hosts[host][ memory ]        lv. cpus = 2      end    end  endendVagrant should now have created a new VM to use as a ByoHost. Now we just need to run the BYOH agent in the VM to make it register as a ByoHost in the management cluster. The BYOH agent needs a kubeconfig file to do this, so we start by copying it to the VM: cp ~/. kube/config ~/. kube/management-cluster. conf# Ensure that the correct IP is used (not localhost)export KIND_IP=$(docker inspect -f '{{range . NetworkSettings. Networks}}{{. IPAddress}}{{end}}' kind-control-plane)sed -i 's/  server\:. */  server\: https\:\/\/' $KIND_IP '\:6443/g' ~/. kube/management-cluster. confscp -i . vagrant/machines/control-plane1/libvirt/private_key \ /home/ubuntu/. kube/management-cluster. conf vagrant@192. 168. 10. 10:management-cluster. confNext, install the prerequisites and host agent in the VM and run it. vagrant sshsudo apt install -y socat ebtables ethtool conntrackwget https://github. com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0. 2. 0/byoh-hostagent-linux-amd64mv byoh-hostagent-linux-amd64 byoh-hostagentchmod +x byoh-hostagentsudo . /byoh-hostagent --namespace metal3 --kubeconfig management-cluster. confYou should now have a management cluster with both the Metal¬≥ and BYOH providers installed, as well as two BareMetalHosts and one ByoHost. $ kubectl -n metal3 get baremetalhosts,byohostsNAME               STATE    CONSUMER  ONLINE  ERROR  AGEbaremetalhost. metal3. io/node-0  available       true       18mbaremetalhost. metal3. io/node-1  available       true       18mNAME                           AGEbyohost. infrastructure. cluster. x-k8s. io/control-plane1  73sCreating a multi-provider cluster: The trick is to create both a Metal3Cluster and a ByoCluster that are owned by one common Cluster. We will use the ByoCluster for the control plane in this case. First the Cluster: apiVersion: cluster. x-k8s. io/v1beta1kind: Clustermetadata: labels:  cni: mixed-cluster-crs-0  crs:  true  name: mixed-clusterspec: clusterNetwork:  pods:   cidrBlocks:    - 192. 168. 0. 0/16  serviceDomain: cluster. local  services:   cidrBlocks:    - 10. 128. 0. 0/12 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1beta1  kind: KubeadmControlPlane  name: mixed-cluster-control-plane infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1beta1  kind: ByoCluster  name: mixed-clusterAdd the rest of the BYOH manifests to get a control plane. The code is collapsed here for easier reading. Please click on the line below to expand it.  KubeadmControlPlane, ByoCluster and ByoMachineTemplate    apiVersion: controlplane. cluster. x-k8s. io/v1beta1kind: KubeadmControlPlanemetadata: labels:  nodepool: pool0 name: mixed-cluster-control-planespec: kubeadmConfigSpec:  clusterConfiguration:   apiServer:    certSANs:     - localhost     - 127. 0. 0. 1     - 0. 0. 0. 0     - host. docker. internal   controllerManager:    extraArgs:     enable-hostpath-provisioner:  true   files:   - content: |     apiVersion: v1     kind: Pod     metadata:      creationTimestamp: null      name: kube-vip      namespace: kube-system     spec:      containers:      - args:       - start       env:       - name: vip_arp        value:  true        - name: vip_leaderelection        value:  true        - name: vip_address        value: 192. 168. 10. 20       - name: vip_interface        value: {{ . DefaultNetworkInterfaceName }}       - name: vip_leaseduration        value:  15        - name: vip_renewdeadline        value:  10        - name: vip_retryperiod        value:  2        image: ghcr. io/kube-vip/kube-vip:v0. 3. 5       imagePullPolicy: IfNotPresent       name: kube-vip       resources: {}       securityContext:        capabilities:         add:         - NET_ADMIN         - SYS_TIME       volumeMounts:       - mountPath: /etc/kubernetes/admin. conf        name: kubeconfig      hostNetwork: true      volumes:      - hostPath:        path: /etc/kubernetes/admin. conf        type: FileOrCreate       name: kubeconfig     status: {}    owner: root:root    path: /etc/kubernetes/manifests/kube-vip. yaml  initConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:     - Swap     - DirAvailable--etc-kubernetes-manifests     - FileAvailable--etc-kubernetes-kubelet. conf  joinConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:     - Swap     - DirAvailable--etc-kubernetes-manifests     - FileAvailable--etc-kubernetes-kubelet. conf machineTemplate:  infrastructureRef:   apiVersion: infrastructure. cluster. x-k8s. io/v1beta1   kind: ByoMachineTemplate   name: mixed-cluster-control-plane replicas: 1 version: v1. 23. 5---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoClustermetadata: name: mixed-clusterspec: bundleLookupBaseRegistry: projects. registry. vmware. com/cluster_api_provider_bringyourownhost bundleLookupTag: v1. 23. 5 controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoMachineTemplatemetadata: name: mixed-cluster-control-planespec: template:  spec: {}   So far this is a ‚Äúnormal‚Äù Cluster backed by the BYOH provider. But now it is time to do something different. Instead of adding more ByoHosts as workers, we will add a Metal3Cluster and MachineDeployment backed by BareMetalHosts!Note that the controlPlaneEndpoint of the Metal3Cluster must point to the same endpoint that the ByoCluster is using. apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3Clustermetadata: name: mixed-clusterspec: controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443 noCloudProvider: true IPPools   apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: provisioning-poolspec: clusterName: mixed-cluster namePrefix: test1-prov pools:  - end: 172. 22. 0. 200   start: 172. 22. 0. 100 prefix: 24---apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: baremetalv4-poolspec: clusterName: mixed-cluster gateway: 192. 168. 111. 1 namePrefix: test1-bmv4 pools:  - end: 192. 168. 111. 200   start: 192. 168. 111. 100 prefix: 24   These manifests are quite large but they are just the same as would be used by the metal3-dev-env with some name changes here and there. The key thing to note is that all references to a Cluster are to the one we defined above. Here is the MachineDeployment: apiVersion: cluster. x-k8s. io/v1beta1kind: MachineDeploymentmetadata: labels:  cluster. x-k8s. io/cluster-name: mixed-cluster  nodepool: nodepool-0 name: test1spec: clusterName: mixed-cluster replicas: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: mixed-cluster   nodepool: nodepool-0 template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: mixed-cluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1beta1     kind: KubeadmConfigTemplate     name: test1-workers   clusterName: mixed-cluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1beta1    kind: Metal3MachineTemplate    name: test1-workers   nodeDrainTimeout: 0s   version: v1. 23. 5Finally, we add the Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate. Here you may want to add your public ssh key in the KubeadmConfigTemplate (the last few lines).  Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate    apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3MachineTemplatemetadata: name: test1-workersspec: template:  spec:   dataTemplate:    name: test1-workers-template   image:    checksum: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img. md5sum    checksumType: md5    format: raw    url: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3DataTemplatemetadata: name: test1-workers-template namespace: metal3spec: clusterName: mixed-cluster metaData:  ipAddressesFromIPPool:   - key: provisioningIP    name: provisioning-pool  objectNames:   - key: name    object: machine   - key: local-hostname    object: machine   - key: local_hostname    object: machine  prefixesFromIPPool:   - key: provisioningCIDR    name: provisioning-pool networkData:  links:   ethernets:    - id: enp1s0     macAddress:      fromHostInterface: enp1s0     type: phy    - id: enp2s0     macAddress:      fromHostInterface: enp2s0     type: phy  networks:   ipv4:    - id: baremetalv4     ipAddressFromIPPool: baremetalv4-pool     link: enp2s0     routes:      - gateway:        fromIPPool: baremetalv4-pool       network: 0. 0. 0. 0       prefix: 0  services:   dns:    - 8. 8. 8. 8---apiVersion: bootstrap. cluster. x-k8s. io/v1beta1kind: KubeadmConfigTemplatemetadata: name: test1-workersspec: template:  spec:   files:    - content: |      network:       version: 2       renderer: networkd       bridges:        ironicendpoint:         interfaces: [enp1s0]         addresses:         - {{ ds. meta_data. provisioningIP }}/{{ ds. meta_data. provisioningCIDR }}     owner: root:root     path: /etc/netplan/52-ironicendpoint. yaml     permissions:  0644     - content: |      [registries. search]      registries = ['docker. io']      [registries. insecure]      registries = ['192. 168. 111. 1:5000']     path: /etc/containers/registries. conf   joinConfiguration:    nodeRegistration:     kubeletExtraArgs:      cgroup-driver: systemd      container-runtime: remote      container-runtime-endpoint: unix:///var/run/crio/crio. sock      feature-gates: AllAlpha=false      node-labels: metal3. io/uuid={{ ds. meta_data. uuid }}      provider-id: metal3://{{ ds. meta_data. uuid }}      runtime-request-timeout: 5m     name:  {{ ds. meta_data. name }}    preKubeadmCommands:    - netplan apply    - systemctl enable --now crio kubelet   users:    - name: metal3     # sshAuthorizedKeys:     # - add your public key here for debugging     sudo: ALL=(ALL) NOPASSWD:ALL   The result of all this is a Cluster with two Machines, one from the Metal¬≥ provider and one from the BYOH provider. $ k -n metal3 get machineNAME                CLUSTER     NODENAME        PROVIDERID                   PHASE   AGE   VERSIONmixed-cluster-control-plane-48qmm  mixed-cluster  control-plane1     byoh://control-plane1/jf5uye          Running  7m41s  v1. 23. 5test1-8767dbccd-24cl5        mixed-cluster  test1-8767dbccd-24cl5  metal3://0642d832-3a7c-4ce9-833e-a629a60a455c  Running  7m18s  v1. 23. 5Let‚Äôs also check that the workload cluster is functioning as expected. Get the kubeconfig and add Calico as CNI. clusterctl get kubeconfig -n metal3 mixed-cluster &gt; kubeconfig. yamlexport KUBECONFIG=kubeconfig. yamlkubectl apply -f https://docs. projectcalico. org/v3. 20/manifests/calico. yamlNow check the nodes. $ kubectl get nodesNAME          STATUS  ROLES         AGE  VERSIONcontrol-plane1     Ready  control-plane,master  88m  v1. 23. 5test1-8767dbccd-24cl5  Ready  &lt;none&gt;         82m  v1. 23. 5Going back to the management cluster, we can inspect the state of the cluster API resources. $ clusterctl -n metal3 describe cluster mixed-clusterNAME                                    READY SEVERITY REASON SINCE MESSAGECluster/mixed-cluster                            True           13m‚îú‚îÄClusterInfrastructure - ByoCluster/mixed-cluster‚îú‚îÄControlPlane - KubeadmControlPlane/mixed-cluster-control-plane      True           13m‚îÇ ‚îî‚îÄMachine/mixed-cluster-control-plane-hp2fp                True           13m‚îÇ  ‚îî‚îÄMachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5‚îî‚îÄWorkers ‚îî‚îÄMachineDeployment/test1                         True           3m57s  ‚îî‚îÄMachine/test1-7f77dfb7c8-j7x4q                    True           9m32sConclusion: As we have seen in this post, it is possible to combine at least some infrastructure providers when creating a single cluster. This can be useful for example if a provider has a high cost or limited resources. Furthermore, the use case is not addressed by MachineDeployments since they would all be from the same provider (even though they can have different properties). There is some room for development and improvement though. The most obvious thing is perhaps that Clusters only have one infrastructureRef. This means that the cluster API controllers are not aware of the ‚Äúsecondary‚Äù infrastructure provider(s). Another thing that may be less obvious is the reliance on Nodes and Machines in the Kubeadm control plane provider. It is not an issue in the example we have seen here since both Metal¬≥ and BYOH creates Nodes. However, there are some projects where Nodes are unnecessary. See for example Kamaji, which aims to integrate with the cluster API. The idea here is to run the control plane components in the management cluster as Pods. Naturally, there would not be any control plane Nodes or Machines in this case. (A second provider would be used to add workers. )But the Kubeadm control plane provider expects there to be both Machines and Nodes for the control plane, so a new provider is likely needed to make this work as desired. This issue can already be seen in the vcluster provider, where the Cluster stays in Provisioning state because it is ‚ÄúWaiting for the first control plane machine to have its status. nodeRef set‚Äù. The idea with vcluster is to reuse the Nodes of the management cluster but provide a separate control plane. This gives users better isolation than just namespaces without the need for another ‚Äúreal‚Äù cluster. It is for example possible to have different custom resource definitions in each vcluster. But since vcluster runs all the pods (including the control plane) in the management cluster, there will never be a control plane Machine or nodeRef. There is already one implementation of a control plane provider without Nodes, i. e. the EKS provider. Perhaps this is the way forward. One implementation for each specific case. It would be nice if it was possible to do it in a more generic way though, similar to how the Kubeadm control plane provider is used by almost all infrastructure providers. To summarize, there is already some support for mixed clusters with multiple providers. However, there are some issues that make it unnecessarily awkward. Two things that could be improved in the cluster API would be the following:  Make the cluster. infrastructureRef into a list to allow multiple infrastructure providers to be registered.  Drop the assumption that there will always be control plane Machines and Nodes (e. g. by implementing a new control plane provider). "
    }, {
    "id": 1,
    "url": "/blog/2021/05/05/Pivoting.html",
    "title": "Metal3 Introduces Pivoting",
    "author" : "Kashif Nizam Khan",
    "tags" : "metal3, baremetal, Pivoting, Move",
    "body": "Metal3 project has introduced pivoting in its CI workflow. The motivation forpivoting is to move all the objects from the ephemeral/managementcluster to a target cluster. This blog post will briefly introduce the conceptof pivoting and the impact it has on the overall CI workflow. For the rest ofthis blog, we refer ephemeral/management cluster as an ephemeral cluster. What is Pivoting?: In the context of Metal3 Provider, Pivoting is the process of movingCluster-API and Metal3 objects from the ephemeral k8s cluster to a targetcluster. In Metal3, this process is performed using theclusterctl toolprovided by Cluster-API. clusterctl recognizes pivoting as a move. During thepivot process, clusterctl pauses any reconciliation of Cluster-API objects andthis gets propagated to Cluster-api-provider-metal3 (CAPM3) objects as well. Once all the objects are paused, the objects are created on the other side onthe target cluster and deleted from the ephemeral cluster. Prerequisites: Prior to the actual pivot process, the target cluster should already have theprovider components, ironic containers and CNI installed and running. To performpivot outside metal3-dev-env, specifically, the following points need to beaddressed:  clusterctl is used to initialize both the ephemeral and target cluster.  BMH objects have correct status annotation.  Maintain connectivity towards the provisioning network.  Baremetal Operator(BMO) is deployed as part of CAPM3.  Objects should have a proper owner reference chain. For a detailed explanation of the above-mentioned prerequisites please read thepivoting documentation. Pivoting workflow in CI: The Metal3 CI currently includes pivoting as part of the deploymentprocess both for Ubuntu and CentOS-based jobs. This essentially means allthe PRs that go in, are tested through the pivoting workflow. Here is theCI deployment workflow:  make the metal3-dev-env. It gives us the ephemeral cluster with all the necessary controllers runningwithin it. The corresponding metal3-dev-env command is make provision target cluster. For normal integration tests, this step deploysa control-plane node and a worker in the target cluster. For, feature-testand feature-test-upgrade the provision step deploys three control-planes and aworker. The corresponding metal3-dev-env commands are (normal integration testworkflow):. /scripts/provision/cluster. sh. /scripts/provision/controlplane. sh. /scripts/provision/worker. sh Initialize the provider components on the target cluster. This installs allthe controllers and associated components related to cluster-api ,cluster-api-provider-metal3, baremetal-operator and ironic. Since it isnecessary to have only one set of ironic deployment/containers in the picture,this step also deletes the ironic deployment/containers fromephemeral cluster.  Move all the objects from ephemeral to the target cluster.  Check the status of the objects to verify whether the objects are beingreconciled correctly by the controllers in the target cluster. This stepverifies and finalizes the pivoting process. The corresponding metal3-dev-envthe command that performs this and the previous two steps is :. /scripts/feature_tests/pivoting/pivot. sh Move the objects back to the ephemeral cluster. This step alsoremoves the ironic deployment from the target cluster and reinstates theironic deployment/containers in the ephemeral cluster. Since we donot delete the provider components in the ephemeral cluster,installing them again is not necessary. The corresponding metal3-dev-env commandthat performs this step is :. /scripts/feature_tests/pivoting/repivot. sh De-provision the BMHs and delete the target cluster. The correspondingmetal3-dev-env commands to de-provision worker, controlplane and the cluster is asfollows:. /scripts/deprovision/worker. sh. /scripts/deprovision/controlplane. sh. /scripts/deprovision/cluster. shNote that, if we de-provision cluster, that would de-provision worker andcontrolplane automatically. Pivoting in Metal3: The pivoting process described above is realized in ansible scriptsmove. ymland move_back. yml. Under the hood, pivoting uses themove command from clusterctlprovided by Cluster-API. As stated earlier, all the PRs that go into any Metal3 repository where theintegration tests are run, the code change introduced in the PR is verified withpivoting also in the integration tests now. Moreover, the upgrade workflow inMetal3 performs all the upgrade operations in Metal3 after pivoting to thetarget cluster. "
    }, {
    "id": 2,
    "url": "/blog/2020/07/06/IP_address_manager.html",
    "title": "Introducing the Metal3 IP Address Manager",
    "author" : "Ma√´l Kimmerlin",
    "tags" : "metal3, baremetal, IPAM, ip address manager",
    "body": "As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4release, the Metal3 crew introduced a new project: its own IP Address Manager. This blog post will go through the motivations behind such a project, thefeatures that it brings, its use in Metal3 and future work. What is the IP Address Manager?: The IP Address Manager (IPAM) is a controller that provides IP addresses andmanages the allocations of IP subnets. It is not a DHCP server in that it onlyreconciles Kubernetes objects and does not answer any DHCP queries. Itallocates IP addresses on request but does not handle any use of thoseaddresses. This sounds like the description of any IPAM system, no? Well, the twistis that this manager is based on Kubernetes to specifically handle someconstraints from Metal3. We will go through the different issues that thisproject tackles. When deploying nodes in a bare metal environment, there are a lot of possiblevariations. This project specifically aims to solve cases where staticIP address configurations are needed. It is designed to specifically addressthis in the Cluster API (CAPI) context. CAPI addresses the deployment of Kubernetes clusters and nodes, usingthe Kubernetes API. As such, it uses objects such as Machine Deployments(similar to deployments for pods) that takes care of creating the requestednumber of machines, based on templates. The replicas can be increased by theuser, triggering the creation of new machines based on the provided templates. This mechanism does not allow for flexibility to be able to provide staticaddresses for each machine. The manager adds this flexibility by providingthe address right before provisioning the node. In addition, all the resources from the source cluster must support the CAPIpivoting, i. e. being copied and recreated in the target cluster. This meansthat all objects must contain all needed information in their spec field torecreate the status in the target cluster without losing information. Allobjects must, through a tree of owner references, be attached to the clusterobject, for the pivoting to proceed properly. In a nutshell, the manager provides an IP Address allocation service, basedon Kubernetes API and fulfilling the needs of Metal3, specifically therequirements of CAPI. How does it work?: The manager follows the same logic as the volume allocation in Kubernetes,with a claim and an object created for that claim. There are three types ofobjects defined, the IPPool, the IPClaim and the IPAddress objects. The IPPool objects contain the definition of the IP subnets from which theAddresses are allocated. It supports both IPv4 and IPv6. The subnets can eitherbe defined as such or given as start and end IP addresses with a prefix. It also supports pre-allocating IP addresses. The following is an example IPPool definition : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 192. 168. 0. 10   end: 192. 168. 0. 30   prefix: 25   gateway: 192. 168. 0. 1  - subnet: 192. 168. 1. 1/26  - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12An IPv6 IPPool would be defined similarly : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 2001:0db8:85a3:0000:0000:8a2e::10   end: 2001:0db8:85a3:0000:0000:8a2e:ffff:fff0   prefix: 96   gateway: 12001:0db8:85a3:0000:0000:8a2e::1  - subnet: 2001:0db8:85a3:0000:0000:8a2d::/96 prefix: 96 gateway: 2001:0db8:85a3:0000:0000:8a2d::1Whenever something requires an IP address from the IPPool, it will create anIPClaim. The IPClaim contains a pointer to the IPPool and an owner referenceto the object that created it. The following is an example of an IPClaim: apiVersion: ipam. metal3. io/v1alpha1kind: IPClaimmetadata: name: claim1spec: pool:  Name: pool1status: address:  Name: pool1-192-168-0-13The controller will then reconcile this object and allocate an IP address. Itwill create an IPAddress object representing the allocated address. It willthen update the IPPool status to list the IP Address and the IPClaim statusto point to the IPAddress. The following is an example of an IPAddress: apiVersion: ipam. metal3. io/v1alpha1kind: IPAddressmetadata: name: pool1-192-168-0-13spec: pool:  Name: pool1 claim:  Name: claim1 address: 192. 168. 0. 13 prefix: 24 gateway: 192. 168. 0. 1After this allocation, the IPPool will be looking like this: apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 192. 168. 0. 10   end: 192. 168. 0. 30   prefix: 25   gateway: 192. 168. 0. 1  - subnet: 192. 168. 1. 1/26  - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12status: indexes:  claim1: 192. 168. 0. 13  claim2: 192. 168. 0. 12Use in Metal3: The IP Address Manager is used in Metal3 together with the metadata and networkdata templates feature. Each Metal3Machine (M3M) and Metal3MachineTemplate(M3MT) is associated with a Metal3DataTemplate that contains metadata and /or a network data template that will be rendered for each Metal3Machine. Therendered data will then be provided to Ironic. Those templates referenceIPPool objects. For each Metal3Machine, an IPClaim is created for eachIPPool, and the templates are rendered with the allocated IPAddress. This is how we achieve dynamic IP Address allocations in setups thatrequire static configuration, allowing us to use Machine Deployment and KubeadmControl Plane objects from CAPI in hardware labs where DHCP is not supported. Since each IPAddress has an owner reference set to its IPClaim object, andIPClaim objects have an owner reference set to the Metal3Data object createdfrom the Metal3DataTemplate, the owner reference chain links a Metal3Machine toall the IPClaim and IPAddress objects were created for it, allowing for CAPIpivoting. What now?: The project is fulfilling its basic requirements, but we are looking intoextending it and covering more use cases. For example, we are looking atadding integration with Infoblox and other external IPAM services. Do nothesitate to open an issue if you have some ideas for new features! The project can be foundhere. "
    }, {
    "id": 3,
    "url": "/blog/2020/07/05/raw-image-streaming.html",
    "title": "Raw image streaming available in Metal3",
    "author" : "Ma√´l Kimmerlin",
    "tags" : "metal3, baremetal, raw image, image streaming",
    "body": "Metal3 supports multiple types of images for deployment, the mostpopular being QCOW2. We have recently added support for a feature of Ironicthat improves deployments on constrained environments, raw image streaming. We‚Äôll first dive into how Ironic deploys the images on the target hosts, andhow raw image streaming improves this process. Afterwards, we will point outthe changes to take this into use in Metal3. Image deployments with Ironic: In Metal3, the image deployment is performed by the Ironic Python Agent (IPA)image running on the target host. In order to deploy an image, Ironic willfirst boot the target node with an IPA image over iPXE. IPA will run in memory. Once IPA runs on the target node, Ironic will instruct it to download thetarget image. In Metal3, we use HTTP(S) for the download of the image. IPA willdownload the image and, depending on the format of the image, prepare it towrite on the disk. This means that the image is downloaded in memory anddecompressed, two steps that can be both time and memory consuming. In order to improve this process, Ironic implemented a feature called raw imagestreaming. What is raw image streaming?: The target image format when writing to disk is raw. That‚Äôs why the images informats like QCOW2 must be processed before being written to disk. However, ifthe image that is downloaded is already in raw format, then no processing isneeded. Ironic leverages this, and instead of first downloading the image and thenprocessing it before writing it to disk, it will directly write thedownloaded image to the disk. This feature is known as image streaming. Image streaming can only be performed with images in raw format. Since the downloaded image when streamed is directly written to disk, thememory size requirements change. For any other format than raw, the targethost needs to have sufficient memory to both run IPA (4GB) anddownload the image in memory. However, with raw images, the only constrainton memory is to run IPA (so 4GB). For example, in order to deploy an Ubuntuimage (around 700MB, QCOW2), the requirement is 8GB when in QCOW2 format, whileit is only 4GB (as for any other image) when streamed as raw. This allowsthe deployment of images that are bigger than the available memory on constrained nodes. However, this shifts the load on the network, since the raw images are usuallymuch bigger than other formats. Using this feature in network constrainedenvironment is not recommended. Raw image streaming in Metal3: In order to use raw image streaming in Metal3, a couple of steps are needed. The first one is to convert the image to raw and make it available in anHTTP server. This can be achieved by running : qemu-img convert -O raw  ${IMAGE_NAME}   ${IMAGE_RAW_NAME} Once converted the image format needs to be provided to Ironic through theBareMetalHost (BMH) image spec field. If not provided, Ironic will assume thatthe format is unspecified and download it in memory first. The following is an example of the BMH image spec field in Metal3 Dev Env. apiVersion: metal3. io/v1alpha1kind: BareMetalHostspec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5If deploying with Cluster API provider Metal3 (CAPM3), CAPM3 takes care ofsetting the image field of BMH properly, based on the image field values inthe Metal3Machine (M3M), which might be based on a Metal3MachineTemplate (M3MT). So in order to use raw image streaming, the format of the image must beprovided in the image spec field of the Metal3Machine or Metal3MachineTemplate. The following is an example of the M3M image spec field in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3Machinespec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5The following is for a M3MT in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3MachineTemplatespec: template:  spec:   image:    format: raw    url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img    checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum    checksumType: md5This will enable raw image streaming. By default, metal3-dev-env uses the raw imagestreaming, in order to minimize the resource requirements of the environment. In a nutshell: With the addition of raw image streaming, Metal3 now supports a wider range ofhardware, specifically, the memory-constrained nodes and speeds up deployments. Metal3 still supports all the other formats it supported until now. This newfeature changes the way raw images are deployed for better efficiency. "
    }, {
    "id": 4,
    "url": "/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html",
    "title": "Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster",
    "author" : "Himanshu Roy",
    "tags" : "metal3, kubernetes, cluster API, metal3-dev-env",
    "body": "Introduction: This blog post describes how to deploy a bare metal cluster, a virtual one for simplicity, using Metal¬≥/metal3-dev-env. We will briefly discuss the steps involved in setting up the cluster as well as some of the customization available. If you want to know more about the architecture of Metal¬≥, this blogpost can be helpful. This post builds upon the detailed metal3-dev-env walkthrough blogpost which describes in detail the steps involved in the environment set-up and management cluster configuration. Here we will use that environment to deploy a new Kubernetes cluster using Metal¬≥. Before we get started, there are a couple of requirements we are expecting to be fulfilled. Requirements:  Metal¬≥ is already deployed and working, if not please follow the instructions in the previously mentioned detailed metal3-dev-env walkthrough blogpost.  The appropriate environment variables are setup via shell or in the config_${user}. sh file, for example -     CAPM3_VERSION   NUM_NODES   CLUSTER_NAME   Overview of Config and Resource types: In this section, we give a brief overview of the important config files and resources used as part of the bare metal cluster deployment. The following sub-sections show the config files and resources that are created and give a brief description of some of them. This will help you understand the technical details of the cluster deployment. You can also choose to skip this section, visit the next section about provisioning first and then revisit this. Config Files and Resources Types: Information Among these the config files are rendered under the path https://github. com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/files as part of the provisioning process.  A description of some of the files part of provisioning a cluster, in a centos-based environment :       Name   Description   Path         provisioning scripts   Scripts to trigger provisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/provision/       deprovisioning scripts   Scripts to trigger deprovisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/deprovision/       templates directory   Templates for cluster, control plane, worker definitions   ${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/templates       clusterctl env file   Cluster parameters and details   ${Manifests}/clusterctl_env_centos. rc       generate templates   Renders cluster, control plane and worker definitions in the Manifest directory   ${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/tasks/generate_templates. yml       main vars file   Variable file that assigns all the defaults used during deployment   ${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/vars/main. yml    Here are some of the resources that are created as part of provisioning :       Name   Description         Cluster   a Cluster API resource for managing a cluster       Metal3Cluster   Corresponding Metal3 resource generated as part of bare metal cluster deployment, and managed by Cluster       KubeadmControlPlane   Cluster API resource for managing the control plane, it also manages the Machine object, and has the KubeadmConfig       MachineDeployment   Cluster API resource for managing workers via MachineSet object, it can be used to add/remove workers by scaling Up/Down       MachineSet   Cluster API resource for managing Machine objects for worker nodes       Machine   Cluster API resource for managing nodes - control plane or workers. In case of Controlplane, its directly managed by KubeadmControlPlane, whereas for Workers it‚Äôs managed by a MachineSet       Metal3Machine   Corresponding Metal3 resource for managing bare metal nodes, it‚Äôs managed by a Machine resource       Metal3MachineTemplate   Metal3 resource which acts as a template when creating a control plane or a worker node       KubeadmConfigTemplate   A template of KubeadmConfig, for Workers, used to generate KubeadmConfig when a new worker node is provisioned   Note The corresponding KubeadmConfig is copied to the control plane/worker at the time of provisioning.  Bare Metal Cluster Deployment: The deployment scripts primarily use ansible and the existing Kubernetes management cluster (based on minikube ) for deploying the bare-metal cluster. Make sure that some of the environment variables used for Metal¬≥ deployment are set, if you didn‚Äôt use config_${user}. sh for setting the environment variables.       Parameter   Description   Default         CAPM3_VERSION   Version of Metal3 API   v1alpha3       POD_CIDR   Pod Network CIDR   192. 168. 0. 0/18       CLUSTER_NAME   Name of bare metal cluster   test1    Steps Involved: All the scripts for cluster provisioning or de-provisioning are located at - ${metal3-dev-env}/scripts/. The scripts call a common playbook which handles all the tasks that are available. The steps involved in the process are :  The script calls an ansible playbook with necessary parameters ( from env variables and defaults ) The playbook executes the role -, ${metal3-dev-env}/vm-setup/roles/v1aX_integration_test, which runs the main task_file for provisioning/deprovisioning the cluster, control plane or a worker There are templates in the role, which are used to render configurations in the Manifest directory. These configurations use kubeadm and are supplied to the Kubernetes module of ansible to create the cluster.  During provisioning, first the clusterctl env file is generated, then the cluster, control plane and worker definition templates for clusterctl are generated at ${HOME}/. cluster-api/overrides/infrastructure-metal3/${CAPM3RELEASE}.  Using the templates generated in the previous step, the definitions for resources related to cluster, control plane and worker are rendered using clusterctl.  Centos or Ubuntu image is downloaded in the next step.  Finally using the above definitions, which are passed to the K8s module in ansible, the corresponding resource( cluster/control plane/worker ) is provisioned.  These same definitions are reused at the time of de-provisioning the corresponding resource, again using the K8s module in ansibleNote The manifest directory is created when provisioning is triggered for the first time and is subsequently used to store the config files that are rendered for deploying the bare metal cluster.  Provision Cluster: This script, located at the path - ${metal3-dev-env}/scripts/provision/cluster. sh, provisions the cluster by creating a Metal3Cluster and a Cluster resource. To see if you have a successful Cluster resource creation( the cluster still doesn‚Äôt have a control plane or workers ), just do : kubectl get Metal3Cluster ${CLUSTER_NAME} -n metal3 This will return the cluster deployed, and you can check the cluster details by describing the returned resource. Here is what a Cluster resource looks like : kubectl describe Cluster ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: Clustermetadata: [. . . . . . ]spec: clusterNetwork:  pods:   cidrBlocks:    - 192. 168. 0. 0/18  services:   cidrBlocks:    - 10. 96. 0. 0/12 controlPlaneEndpoint:  host: 192. 168. 111. 249  port: 6443 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1alpha3  kind: KubeadmControlPlane  name: bmetalcluster  namespace: metal3 infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3Cluster  name: bmetalcluster  namespace: metal3status: infrastructureReady: true phase: Provisioned Provision Controlplane: This script, located at the path - ${metal3-dev-env}/scripts/provision/controlplane. sh, provisions the control plane member of the cluster using the rendered definition of the control plane explained in the Steps Involved section. The KubeadmControlPlane creates a Machine which picks up a BareMetalHost satisfying its requirements as the control plane node, and it is then provisioned by the Bare Metal Operator. A Metal3MachineTemplate resource is also created as part of the provisioning process. Note It takes some time for the provisioning of the control plane, you can watch the process using some steps shared a bit later kubectl get KubeadmControlPlane ${CLUSTER_NAME} -n metal3kubectl describe KubeadmControlPlane ${CLUSTER_NAME} -n metal3apiVersion: controlplane. cluster. x-k8s. io/v1alpha3kind: KubeadmControlPlanemetadata: [. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  blockOwnerDeletion: true  controller: true  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  44555  selfLink: /apis/controlplane. cluster. x-k8s. io/v1alpha3/namespaces/metal3/kubeadmcontrolplanes/bmetalcluster uid: 99487c75-30f1-4765-b895-0b83b0e5402bspec: infrastructureTemplate:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3MachineTemplate  name: bmetalcluster-controlplane  namespace: metal3 kubeadmConfigSpec:  files:  - content: |    [. . . . ] replicas: 1 version: v1. 18. 0status: replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,cluster. x-k8s. io/control-plane= unavailableReplicas: 1 updatedReplicas: 1kubectl get Metal3MachineTemplate ${CLUSTER_NAME}-controlplane -n metal3 To track the progress of provisioning, you can try the following: kubectl get BareMetalHosts -n metal3 -w The BareMetalHosts resource is created when Metal¬≥/metal3-dev-env was deployed. It is a kubernetes resource that represents a bare metal Machine, with all its details and configuration, and is managed by the Bare Metal Operator. You can also use the short representation instead, i. e. bmh ( short for BareMetalHosts) in the command above.  You should see all the nodes that were created at the time of metal3 deployment, along with their current status as the provisioning progresses Note All the bare metal hosts listed above were created when Metal¬≥ was deployed in the detailed metal3-dev-env walkthrough blogpost. kubectl get Machine -n metal3 -w This shows the status of the Machine associated with the control plane and we can watch the status of provisioning under PHASE Once the provisioning is finished, let‚Äôs get the host-ip : sudo virsh net-dhcp-leases baremetalInformation baremetal is one of the 2 networks that were created at the time of Metal3 deployment, the other being ‚Äúprovisioning‚Äù which is used - as you have guessed - for provisioning the bare metal cluster. More details about networking setup in the metal3-dev-env environment are described in the - detailed metal3-dev-env walkthrough blogpost. You can log in to the control plane node if you want, and can check the deployment status using two methods. ssh metal3@{control-plane-node-ip}ssh metal3@192. 168. 111. 249 Provision Workers: The script is located at ${metal3-dev-env-path}/scripts/provision/worker. sh and it provisions a node to be added as a worker to the bare metal cluster. It selects one of the remaining nodes and provisions it and adds it to the bare metal cluster ( which only has a control plane node at this point ). The resources created for workers are - MachineDeployment which can be scaled up to add more workers to the cluster and MachineSet which then creates a Machine managing the node. Information Similar to control plane provisioning, worker provisioning also takes some time, and you can watch the process using steps shared a bit later. This will also apply when you scale Up/Down workers at a later point in time. This is what a MachineDeployment looks like kubectl describe MachineDeployment ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: MachineDeploymentmetadata: [. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  66257  selfLink: /apis/cluster. x-k8s. io/v1alpha3/namespaces/metal3/machinedeployments/bmetalcluster uid: f598da43-0afe-44e4-b793-cd5244c13f4espec: clusterName: bmetalcluster minReadySeconds: 0 progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: bmetalcluster   nodepool: nodepool-0 strategy:  rollingUpdate:   maxSurge: 1   maxUnavailable: 0  type: RollingUpdate template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: bmetalcluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1alpha3     kind: KubeadmConfigTemplate     name: bmetalcluster-workers   clusterName: bmetalcluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3    kind: Metal3MachineTemplate    name: bmetalcluster-workers   version: v1. 18. 0status: observedGeneration: 1 phase: ScalingUp replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,nodepool=nodepool-0 unavailableReplicas: 1 updatedReplicas: 1To check the status we can follow steps similar to Controlplane case : kubectl get bmh -n metal3 -w We can see the live status of the node being provisioned. As mentioned before bmh is the short representation of BareMetalHosts. kubectl get Machine -n metal3 -w This shows the status of Machines associated with workers, apart from the one for Controlplane, and we can watch the status of provisioning under PHASE sudo virsh net-dhcp-leases baremetal To get the node‚Äôs IP ssh metal3@{control-plane-node-ip}kubectl get nodes To check if it‚Äôs added to the cluster ssh metal3@{node-ip} If you want to log in to the node kubectl scale --replicas=3 MachineDeployment ${CLUSTER_NAME} -n metal3 We can add or remove workers to the cluster, and we can scale up the MachineDeployment up or down, in this example we are adding 2 more worker nodes, making the total nodes = 3 Deprovisioning: All of the previous components have corresponding de-provisioning scripts which use config files, in the previously mentioned manifest directory, and use them to clean up the worker, control plane and cluster. This step will use the already generated cluster/control plane/worker definition file, and supply it to Kubernetes ansible module to remove/de-provision the resource. You can find it, under the Manifest directory, in the Snapshot shared at the beginning of this blogpost where we show the file structure. For example, if you wish to de-provision the cluster, you would do : sh ${metal3-dev-env-path}/scripts/deprovision/worker. shsh ${metal3-dev-env-path}/scripts/deprovision/controlplane. shsh ${metal3-dev-env-path}/scripts/deprovision/cluster. shNote The reason for running the deprovision/worker. sh and deprovision/controlplane. sh scripts is that not all objects are cleared when we just run the deprovision/cluster. sh script. Following this, if you want to de-provision the control plane it is recommended to de-provision the cluster itself since we can‚Äôt provision a new control plane with the same cluster. For worker de-provisioning, we only need to run the worker script. The following video demonstrates all the steps to provision and de-provision a Kubernetes cluster explained above. Summary: In this blogpost we saw how to deploy a bare metal cluster once we have a Metal¬≥(metal3-dev-env repo) deployed and by that point we will already have the nodes ready to be used for a bare metal cluster deployment. In the first section, we show the various configuration files, templates, resource types and their meanings. Then we see the common steps involved in the provisioning process. After that, we see a general overview of how all resources are related and at what point are they created - provision cluster/control plane/worker. In each of the provisioning sections, we see the steps to monitor the provisioning and how to confirm if it‚Äôs successful or not, with brief explanations wherever required. Finally, we see the de-provisioning section which uses the resource definitions generated at the time of provisioning to de-provision cluster, control plane or worker. Here are a few resources which you might find useful if you want to explore further, some of them have already been shared earlier.  Metal3-Documentation     Metal3-Try-it    Metal¬≥/metal3-dev-env Detailed metal3-dev-env walkthrough blogpost Kubernetes Metal3 Talk Metal3-Docs-github"
    }, {
    "id": 5,
    "url": "/blog/2020/03/05/CAPI_provider_renaming.html",
    "title": "Cluster API provider renaming",
    "author" : "Ma√´l Kimmerlin",
    "tags" : "metal3, baremetal, cluster API, provider",
    "body": "Renaming of Cluster API provider: Backwards compatibility for v1alpha3 There is no backwards compatibility between v1alpha3 and v1alpha2 releases ofthe Cluster API provider for Metal3. For the v1alpha3 release of Cluster API, the Metal3 provider was renamed fromcluster-api-provider-baremetal to cluster-api-provider-metal3. The CustomResource Definitions were also modified. This post dives into the changes. Repository renaming: From v1alpha3 onwards, the Cluster API provider will be developed incluster-api-provider-metal3. The v1alpha1 and v1alpha2 content will remain incluster-api-provider-baremetal. This repository will be archived but kept for the integration in metal3-dev-env. Custom Resource Definition modifications: The kind of Custom Resource Definition (CRD) has been modified for thefollowing objects:  BareMetalCluster -&gt; Metal3Cluster baremetalcluster -&gt; metal3cluster BareMetalMachine -&gt; Metal3Machine baremetalmachine -&gt; metal3machine BareMetalMachineTemplate -&gt; Metal3MachineTemplate baremetalmachinetemplate -&gt; metal3machinetemplateThe custom resources deployed need to be modified accordingly. Deployment modifications: The prefix of all deployed components for the Metal3 provider was modifiedfrom capbm- to capm3-. The namespace in which the components are deployed bydefault was modified from capbm-system to capm3-system. "
    }, {
    "id": 6,
    "url": "/blog/2020/02/27/talk-kubernetes-finland-metal3.html",
    "title": "Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, talk, conference, kubernetes, meetup",
    "body": "Conference talk: Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin: On the 20th of January at the Kubernetes and CNCF Finland Meetup, Ma√´l Kimmerlin gave a brilliant presentation about the status of the Metal¬≥ project. In this presentation, Ma√´l starts giving a short introduction of the Cluster API project which provides a solid foundation to develop the Metal¬≥ Bare Metal Operator (BMO). The talk basically focuses on the v1alpha2 infrastructure provider features from the Cluster API. Information The video recording from the ‚ÄúKubernetes and CNCF Finland Meetup‚Äù is composed of three talks. The video embedded starts with Ma√´l‚Äôs talk. Warning Playback of the video has been disabled by the author. Click on the play button and then on the ‚ÄúWatch this video on Youtube‚Äù link once it appears.  During the first part of the presentation, a detailed explanation of the different Kubernetes Custom Resource Definitions (CRDs) inside Metal¬≥ is shown as also how they are linked with the Cluster API project. As an example, the image below shows the interaction between objects and controllers from both projects: Once finished the introductory part, Ma√´l focuses on the main components of the Metal¬≥ BMO and the provisioning process. This process starts with introspection, where the bare metal server is registered by the operator. Then, the Ironic Python Agent (IPA) image is executed to collect all hardware information from the server.  The second part of the process is the provisioning. In this step, Ma√´l explains how the Bare Metal Operator (BMO) is in charge along with Ironic to present the Operating System image to the physical server and complete its installation.  Next, Ma√´l deeply explains each Custom Resource (CR) used during the provisioning of target Kubernetes clusters in bare metal servers. He refers to objects such as Cluster, BareMetalCluster, Machine, BareMetalMachine, BareMetalHost and so on. Each one is clarified with a YAML file definition of a real case and a workflow diagram that shows the reconciliation procedure. The last part of the talk is dedicated to executing a demo where Ma√´l creates a target Kubernetes cluster from a running minikube VM (also called bootstrap cluster) where Metal¬≥ is deployed. As it is pointed out in the video, the demo is running in emulated hardware. Actually, something similar to the metal3-dev-env project can be used to reproduce the demo. More information on the Metal¬≥ development environment (metal3-dev-env) can be found in the Metal¬≥ try-it section. In case you want to go deeper, take a look at the blog post A detailed walkthrough of the Metal¬≥ development environment. In the end, the result is a new Kubernetes cluster up and running. The cluster is deployed on two emulated physical servers: one runs as the control-plane node and the other as a worker node. Information The slides of the talk can be downloaded from here Speakers: Ma√´l Kimmerlin Ma√´l Kimmerlin is a Senior Software Engineer at Ericsson. In his own words: I am an open-source enthusiast, focusing in Ericsson on Life Cycle Management of Kubernetes clusters on Bare Metal. I am very interested in the Cluster API project from the Kubernetes Lifecycle SIG, and active in its Bare Metal provider, that is Metal¬≥, developing and encouraging the adoption of this project. References:  Video: Metal¬≥: Kubernetes Native Bare Metal Cluster Management Slides"
    }, {
    "id": 7,
    "url": "/blog/2020/02/18/metal3-dev-env-install-deep-dive.html",
    "title": "A detailed walkthrough of the Metal¬≥ development environment",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction to metal3-dev-env: The metal3-dev-env is a collection of scripts in a GitHub repository inside the Metal¬≥ project that aims to allow contributors and other interested users to run a fully functional Metal¬≥ environment for testing and have a first contact with the project. Actually, metal3-dev-env sets up an emulated environment which creates a set of virtual machines (VMs) to manage as if they were bare metal hosts. Warning This is not an installation that is supposed to be run in production. Instead, it is focused on providing a development environment to test and validate new features. The metal3-dev-env repository includes a set of scripts, libraries and resources used to set up a Metal¬≥ development environment. On the Metal¬≥ website there is already a documented process on how to use the metal3-dev-env scripts to set up a fully functional cluster to test the functionality of the Metal¬≥ components. This procedure at a 10,000-foot view is composed of 3 bash scripts plus a verification one:  01_prepare_host. sh - Mainly installs all needed packages.  02_configure_host. sh - Basically create a set of VMs that will be managed as if they were bare metal hosts. It also downloads some images needed for Ironic.  03_launch_mgmt_cluster. sh - Launches a management cluster using minikube and runs the baremetal-operator on that cluster.  04_verify. sh - Finally runs a set of tests that verify that the deployment was completed successfullyIn this blog post, we are going to expand the information and provide some hints and recommendations. Warning Metal¬≥ project is changing rapidly, so probably this information is valuable in the short term. In any case, it is encouraged to double-check that the information provided is still valid. Before getting down to it, it is worth defining the nomenclature used in the blog post:  Host. It is the server where the virtual environment is running. In this case, it is a physical PowerEdge M520 with 2 x Intel(R) Xeon(R) CPU E5-2450 v2 @ 2. 50GHz, 96GB RAM and a 140GB drive running CentOS 7 latest. Do not panic, lab environment should work with lower resources as well.  Virtual bare metal hosts. These are the virtual machines (KVM based) that are running on the host which are emulating physical hosts in our lab. They are also called bare metal hosts even if they are not physical servers.  Management or bootstrap cluster. It is a fully functional Kubernetes cluster in charge of running all the necessary Metal¬≥ operators and controllers to manage the infrastructure. In this case it is the minikube virtual machine.  Target cluster. It is the Kubernetes cluster created from the management one. It is provisioned and configured using a native Kubernetes API for that purpose. Create the Metal¬≥ laboratory: Information A non-root user must exist in the host with password-less sudo access. This user is in charge of running the metal3-dev-env scripts. The first thing that needs to be done is, obviously, cloning the metal3-dev-env repository: [alosadag@eko1: ~]$ git clone https://github. com/metal3-io/metal3-dev-env. gitCloning into 'metal3-dev-env'. . . remote: Enumerating objects: 22, done. remote: Counting objects: 100% (22/22), done. remote: Compressing objects: 100% (22/22), done. remote: Total 1660 (delta 8), reused 8 (delta 0), pack-reused 1638Receiving objects: 100% (1660/1660), 446. 08 KiB | 678. 00 KiB/s, done. Resolving deltas: 100% (870/870), done. Before starting to deploy the Metal¬≥ environment, it makes sense to detail a series of scripts inside the library folder that will be sourced in every step of the installation process. They are called shared libraries. [alosadag@eko1:~]$ ls -1 metal3-dev-env/lib/common. shimages. shlogging. shnetwork. shShared libraries: Although there are several scripts placed inside the lib folder that are sourced in some of the deployment steps, common. sh and logging. sh are the only ones used in all of the executions during the installation process. common. sh: The first time this library is run, a new configuration file is created with several variables along with their default values. They will be used during the installation process. On the other hand, if the file already exists, then it just sources the values configured. The configuration file is created inside the cloned folder with config_$USER as the file name. [alosadag@eko1 metal3-dev-env]$ ls config_*config_alosadag. shThe configuration file contains multiple variables that will be used during the set-up. Some of them are detailed in the setup section of the Metal¬≥ try-it web page. In case you need to add or change global variables it should be done in this config file. Note I personally recommend modifying or adding variables in this config file instead of exporting them in the shell. By doing that, it is assured that they are persisted [alosadag@eko1 metal3-dev-env]$ cat ~/metal3-dev-env/config_alosadag. sh#!/bin/bash## This is the subnet used on the  baremetal  libvirt network, created as the# primary network interface for the virtual bare metalhosts. ## Default of 192. 168. 111. 0/24 set in lib/common. sh##export EXTERNAL_SUBNET= 192. 168. 111. 0/24 ## This SSH key will be automatically injected into the provisioned host# by the provision_host. sh script. ## Default of ~/. ssh/id_rsa. pub is set in lib/common. sh##export SSH_PUB_KEY=~/. ssh/id_rsa. pub. . . This common. sh library also makes sure there is an ssh public key available in the user‚Äôs ssh folder. This key will be injected by cloud-init in all the virtual bare metal machines that will be configured later. Then, the user that executed the metal3-dev-env scripts is able to access the target cluster through ssh. Also, common. sh library also sets more global variables apart from those in the config file. Note that these variables can be added to the config file along with the proper values for your environment.       Name of the variable   Default value         SSH_KEY   ${HOME}/. ssh/id_rsa       SSH_PUB_KEY   ${SSH_KEY}. pub       NUM_NODES   2       VM_EXTRADISKS   false       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest       VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       IRONIC_INSPECTOR_IMAGE   quay. io/metal3-io/ironic-inspector       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator       CAPM3_VERSION   v1alpha3       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal:v1alpha1       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal       DEFAULT_HOSTS_MEMORY   8192       CLUSTER_NAME   test1       KUBERNETES_VERSION   v1. 17. 0       KUSTOMIZE_VERSION   v3. 2. 3   Information It is important to mention that there are several basic functions defined in this file that will be used by the rest of scripts. logging. sh: This script ensures that there is a log folder where all the information gathered during the execution of the scripts is stored. If there is any issue during the deployment, this is one of the first places to look at. [alosadag@eko1 metal3-dev-env]$ ls -1 logs/01_prepare_host-2020-02-03-122452. log01_prepare_host-2020-02-03-122956. loghost_cleanup-2020-02-03-122656. logFirst step: Prepare the host: In this first step (01_prepare_host. sh), the requirements needed to start the preparation of the host where the virtual bare metal hosts will run are fulfilled. Depending on the host‚Äôs operating system (OS), it will trigger a specific script for CentOS/Red Hat or Ubuntu.  note: ‚ÄúNote‚ÄùCurrently CentOS Linux 7, Red Hat Enterprise Linux 8 and Ubuntu have been tested. There is work in progress to adapt the deployment for CentOS Linux 8. As stated previously, CentOS 7 is the operating system chosen to run in both, the host and virtual servers. Therefore, specific packages of the operating system are applied in the following script:    centos_install_requirements. sh  This script enables epel and tripleo (current-tripleo) repositories where several packages are installed: dnf, ansible, wget, python3 and python related packages such as python-virtualbmc from tripleo repository. Note Notice that SELinux is set to permissive and an OS update is triggered, which will cause several packages to be upgraded since there are newer packages in the tripleo repositories (mostly python related) than in the rest of enabled repositories. At this point, the container runtime is also installed. Note that by setting the variable CONTAINER_RUNTIME defined in common. sh is possible to choose between docker and podman, which is the default for CentOS. Remember that this behaviour can be overwritten in your config file. Once the specific requirements for the elected operating system are accomplished, the download of several external artifacts is executed. Actually minikube, kubectl and kustomize are downloaded from the internet. Notice that the version of Kustomize and Kubernetes is defined by KUSTOMIZE_VERSION and KUBERNETES_VERSION variables inside common. sh, but minikube is always downloading the latest version available. The next step deals with cleaning ironic containers and pods that could be running in the host from failed deployments. This will ensure that there will be no issues when creating ironic-pod and infra-pod a little bit later in this first step.    network. sh.   At this point, the network library script is sourced. As expected, this library deals with the network configuration which includes: IP addresses, network definitions and IPv6 support which is disabled by default by setting PROVISIONING_IPV6 variable:             Name of the variable    Default value    Option              PROVISIONING_NETWORK    172. 22. 0. 0/24    This is the subnet used to run the OS provisioning process          EXTERNAL_SUBNET    192. 168. 111. 0/24    This is the subnet used on the ‚Äúbaremetal‚Äù libvirt network, created as the primary network interface for the virtual bare metal hosts          LIBVIRT_FIRMWARE    bios    ¬†          PROVISIONING_IPV6    false    ¬†       Below it is depicted a network diagram of the different virtual networks and virtual servers involved in the Metal¬≥ environment:    images. sh.   The images. sh library file is sourced as well in script 01_prepare_host. sh. The images. sh script contains multiple variables that set the URL (IMAGE_LOCATION), name (IMAGE_NAME) and default username (IMAGE_USERNAME) of the cloud image that needs to be downloaded. The values of each variable will differ depending on the operating system of the virtual bare metal hosts. Note that these images will be served from the host to the virtual servers through the provisioning network.  In our case, since CentOS 7 is the base operating system, values will be defined as:             Name of the variable    Default value              IMAGE_NAME    CentOS-7-x86_64-GenericCloud-1907. qcow2          IMAGE_LOCATION    http://cloud. centos. org/centos/7/images          IMAGE USERNAME    centos      Information In case it is expected to use a custom cloud image, just modify the previous variables to match the right location. Now that the cloud image is defined, the download process can be started. First, a folder defined by IRONIC_IMAGE_DIR should exist so that the image (CentOS-7-x86_64-GenericCloud-1907. qcow2) and its checksum can be stored. This folder and its content will be exposed through a local ironic container running in the host.       Name of the variable   Default value       IRONIC_IMAGE_DIR   /opt/metal3-dev-env/ironic/html/images   Below it is verified that the cloud image files were downloaded successfully in the defined folder: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumOnce the shared script images. sh is sourced, the following container images are pre-cached locally to the host in order to speed up things later. Below is shown the code snippet in charge of that task: + for IMAGE_VAR in IRONIC_IMAGE IPA_DOWNLOADER_IMAGE VBMC_IMAGE SUSHY_TOOLS_IMAGE DOCKER_REGISTRY_IMAGE+ IMAGE=quay. io/metal3-io/ironic+ sudo podman pull quay. io/metal3-io/ironic. . . . . . . The container image location of each one is defined by their respective variables:       Name of the variable   Default value         VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest   Information In case it is expected to modify the public container images to test new features, it is worth mentioning that there is a container registry running as a privileged container in the host. Therefore it is recommended to upload your modified images there and just overwrite the previous variables to match the right location. At this point, an Ansible role is run locally in order to complete the local configuration. ansible-playbook \ -e  working_dir=$WORKING_DIR  \ -e  virthost=$HOSTNAME  \ -i vm-setup/inventory. ini \ -b -vvv vm-setup/install-package-playbook. ymlThis playbook imports two roles. One is called packages_installation, which is in charge of installing a few more packages. The list of packages installed are listed as default Ansible variables in the vm-setup role inside the metal3-dev-env repository. The other role is based on the fubarhouse. golang Ansible Galaxy role. It is in charge of installing and configuring the exact golang version 1. 12. 12 defined in an Ansible variable in the install-package-playbook. yml playbook Once the playbook is finished, a pod called ironic-pod is created. Inside that pod, a privileged ironic-ipa-downloader container is started and attached to the host network. This container is in charge of downloading the Ironic Python Agent (IPA) files to a shared volume defined by IRONIC_IMAGE_DIR. This folder is exposed by the ironic container through HTTP. Information The Ironic Python Agent is an agent for controlling and deploying Ironic controlled baremetal nodes. Typically run in a ramdisk, the agent exposes a REST API for provisioning servers. See below the code snippet that fulfils the task: sudo podman run -d --net host --privileged --name ipa-downloader --pod ironic-pod -e IPA_BASEURI= -v /opt/metal3-dev-env/ironic:/shared quay. io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource. shBelow is shown the status of the pods and containers at this point: [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407The process will wait until the ironic-python-agent (IPA) initramfs, kernel and headers files are downloaded successfully. See below the files downloaded along with the CentOS 7 cloud image: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumdrwxr-xr-x. 2 root   root      147 Feb 3 12:41 ironic-python-agent-1862d000-59d9fdc6304b1lrwxrwxrwx. 1 root   root      72 Feb 3 12:41 ironic-python-agent. initramfs -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. initramfslrwxrwxrwx. 1 root   root      69 Feb 3 12:41 ironic-python-agent. kernel -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. kernellrwxrwxrwx. 1 root   root      74 Feb 3 12:41 ironic-python-agent. tar. headers -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. tar. headersAfterwards, the script makes sure that libvirt is running successfully on the host and that the non-privileged user has permission to interact with it. Libvirt daemon should be running so that minikube can be installed successfully. See the following script snippet starting the minikube VM: + sudo su -l -c 'minikube start --insecure-registry 192. 168. 111. 1:5000'* minikube v1. 6. 2 on Centos 7. 7. 1908* Selecting 'kvm2' driver from user configuration (alternates: [none])In the same way, as with the host, container images are pre-cached but in this case inside minikube local image repository. Notice that in this case the Bare Metal operator (BMO) is also downloaded since it will run on minikube. The container location is defined by BAREMETAL_OPERATOR_IMAGE. In case you want to test new features or new fixes to the BMO, just change the value of the variable to match the location of the modified image:       Name of the variable   Default value       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator   Note Remember that minikube is the management cluster in our environment. So it must run all the operators and controllers needed for Metal¬≥. Below is shown the output of the script once all the container images have been pulled to minikube: + sudo su -l -c 'minikube ssh sudo docker image ls' alosadagREPOSITORY                TAG         IMAGE ID      CREATED       SIZEquay. io/metal3-io/ironic         latest       e5d81adf05ee    26 hours ago    693MBquay. io/metal3-io/ironic-ipa-downloader  latest       d55b0dac2144    6 days ago     239MBquay. io/metal3-io/ironic-inspector    latest       8bb5b844ada6    6 days ago     408MBquay. io/metal3-io/baremetal-operator   latest       3c692a32ddd6    9 days ago     1. 77GBk8s. gcr. io/kube-proxy           v1. 17. 0       7d54289267dc    7 weeks ago     116MBk8s. gcr. io/kube-controller-manager    v1. 17. 0       5eb3b7486872    7 weeks ago     161MBk8s. gcr. io/kube-scheduler         v1. 17. 0       78c190f736b1    7 weeks ago     94. 4MBk8s. gcr. io/kube-apiserver         v1. 17. 0       0cae8d5cc64c    7 weeks ago     171MBkubernetesui/dashboard          v2. 0. 0-beta8    eb51a3597525    7 weeks ago     90. 8MBk8s. gcr. io/coredns            1. 6. 5        70f311871ae1    2 months ago    41. 6MBk8s. gcr. io/etcd              3. 4. 3-0       303ce5db0e90    3 months ago    288MBkubernetesui/metrics-scraper       v1. 0. 2       3b08661dc379    3 months ago    40. 1MBk8s. gcr. io/kube-addon-manager       v9. 0. 2       bd12a212f9dc    6 months ago    83. 1MBk8s. gcr. io/pause             3. 1         da86e6ba6ca1    2 years ago     742kBgcr. io/k8s-minikube/storage-provisioner  v1. 8. 1       4689081edb10    2 years ago     80. 8MBOnce the container images are stored, minikube can be stopped. At that moment, the virtual networks shown in the previous picture are attached to the minikube VM as can be verified by the following command: [alosadag@smc-master metal3-dev-env]$ sudo virsh domiflist minikubeInterface Type    Source   Model    MAC--------------------------------------------------------     network  default  virtio   d4:38:25:25:c6:ca-     network  minikube-net virtio  a4:c2:8a:9d:2a:d8-     network  provisioning virtio  52:54:00:c8:50:97-     network  baremetal virtio   52:54:00:17:b4:ecInformation At this point the host is ready to create the virtual infrastructure. The video below exhibits all the configurations explained and executed during this first step. Step 2: Configure the host: In this step, the script 02_configure_host. sh basically configures the libvirt/KVM virtual infrastructure and starts services in the host that will be consumed by the virtual bare metal machines:  Web server to expose the ironic-python-agent (IPA) initramfs, kernel, headers and operating system cloud images.  Virtual BMC to emulate a real baseboard management controller (BMC).  Container registry where the virtual servers will pull the images needed to run a K8s installation. Information A baseboard management controller (BMC) is a specialized service processor that monitors the physical state of a computer, network server or other hardware device using sensors and communicating with the system administrator through an independent connection. The BMC is part of the Intelligent Platform Management Interface (IPMI) and is usually contained in the motherboard or main circuit board of the device to be monitored. First, an ssh-key in charge of communicating to libvirt is created if it does not exist previously. This key is called id_rsa_virt_power. It is added to the root authorized_keys and is used by vbmc and sushy tools to contact libvirt. Information sushy-tools is a set of simple simulation tools aiming at supporting the development and testing of the Redfish protocol implementations. Next, another Ansible playbook called setup-playbook. yml is run against the host. It is focused on setting up the virtual infrastructure around metal3-dev-env. Below it is shown the Ansible variables that are passed to the playbook, which actually are obtaining the values from the global variables defined in the common. sh or the configuration file. ANSIBLE_FORCE_COLOR=true ansible-playbook \  -e  working_dir=$WORKING_DIR  \  -e  num_nodes=$NUM_NODES  \  -e  extradisks=$VM_EXTRADISKS  \  -e  virthost=$HOSTNAME  \  -e  platform=$NODES_PLATFORM  \  -e  libvirt_firmware=$LIBVIRT_FIRMWARE  \  -e  default_memory=$DEFAULT_HOSTS_MEMORY  \  -e  manage_baremetal=$MANAGE_BR_BRIDGE  \  -e  provisioning_url_host=$PROVISIONING_URL_HOST  \  -i vm-setup/inventory. ini \  -b -vvv vm-setup/setup-playbook. yml      Name of the variable   Default value         WORKING_DIR   /opt/metal3-dev-env       NUM_NODES   2       VM_EXTRADISKS   false       HOSTNAME   hostname       NODES_PLATFORM   libvirt       LIBVIRT_FIRMWARE   bios       DEFAULT_HOSTS_MEMORY   8192       MANAGE_BR_BRIDGE   y       PROVISIONING_URL_HOST   172. 22. 0. 1   Information There are variables that are only defined as Ansible variables, e. g. number of CPUs of the virtual bare metal server, size of disks, etc. In case you would like to change properties not defined globally in the metal3-dev-env take a look at the default variables specified in role: common and libvirt The setup-playbook. yml is composed by 3 roles, which are detailed below:    Common.   This role sets up the virtual hardware and network configuration of the VMs. Actually it is a dependency of the libvirt and virtbmc Ansible roles. This means that the common role must always be executed before the roles that depend on them. Also, they are only executed once. If two roles state the same one as their dependency, it is only executed the first time.      Libvirt.   It actually is the role that configures the virtual bare metal servers. They are all identically defined with the same hardware and network configuration. Note that they are not started since they will be booted later by ironic during the provisioning process. Note It is possible to change the number of VMs to provision by replacing the value of NUMBER_NODES  Finally, once the VMs are defined and we have their MAC address, the ironic inventory file ironic_nodes_json is created. The action of creating a node is part of the enrollment process and the first step to prepare a node to reach the available status. {  nodes : [  {    name :  node-0 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6230 ,     address :  ipmi://192. 168. 111. 1:6230 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8b ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },  {    name :  node-1 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6231 ,     address :  ipmi://192. 168. 111. 1:6231 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8f ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },Information This role is also used to tear down the virtual infrastructure depending on the variable libvirt_action inside the Ansible role: setup or teardown.    VirtBMC  This role is only executed if the bare metal virtual machines are created in libvirt, because vbmc needs libvirt to emulate a real BMC. Information VirtualBMC (vmbc) tool simulates a Baseboard Management Controller (BMC) by exposing IPMI responder to the network and talking to libvirt at the host vBMC is running at. Basically, manipulate virtual machines which pretend to be bare metal servers.  The virtbmc Ansible role creates the vbmc and sushy-tools configuration in the host for each virtual bare metal nodes. Note that each virtual bare metal host will have a different vbmc socket exposed in the host. The communication to each vbmc is needed by the BMO to start, stop, configure the boot order, etc during the provisioning stage. Finally, this folders containing the configuration will be mounted by the vbmc and sushy-tools containers.    [alosadag@eko1 metal3-dev-env]$ sudo ls -l --color /opt/metal3-dev-env/virtualbmctotal 0drwxr-x---. 2 root root 21 Feb 5 11:07 sushy-toolsdrwxr-x---. 4 root root 70 Feb 5 11:08 vbmc Next, both host provisioning and baremetal interfaces are configured. The provisioning interface, as the name suggests, will be used to provision the virtual bare metal hosts by means of the `Bare Metal Operator`. This interface is configured with an static IP (172. 22. 0. 1):```sh[alosadag@smc-master metal3-dev-env]$ ifconfig provisioningprovisioning: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500  inet 172. 22. 0. 1 netmask 255. 255. 255. 0 broadcast 172. 22. 0. 255  inet6 fe80::1091:c1ff:fea1:6a0f prefixlen 64 scopeid 0x20&lt;link&gt;  ether 12:91:c1:a1:6a:0f txqueuelen 1000 (Ethernet)On the other hand, the baremetal virtual interface behaves as an external network. This interface is able to reach the internet and it is the network where the different Kubernetes nodes will exchange information. This interface is configured as auto, so the IP is retrieved by DHCP. [alosadag@smc-master metal3-dev-env]$ ifconfig baremetalbaremetal: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500  inet 192. 168. 111. 1 netmask 255. 255. 255. 0 broadcast 192. 168. 111. 255  ether 52:54:00:db:85:29 txqueuelen 1000 (Ethernet)Next, an Ansible role called firewall will be executed targetting the host to be sure that the proper ports are opened. In case your host is running Red Hat Enterprise Linux or CentOS 8, firewall module will be used. In any other case, iptables module is the choice. Below is the code snippet where firewalld or iptables is assigned: # Use firewalld on CentOS/RHEL, iptables everywhere elseexport USE_FIREWALLD=Falseif [[ ($OS ==  rhel  || $OS =  centos ) &amp;&amp; ${OS_VERSION} == 8 ]]then export USE_FIREWALLD=TruefiNote This behaviour can be changed by replacing the value of the USE_FIREWALLD variable The ports managed by this role are all associated with the services that take part in the provisioning process: ironic, vbmc, httpd, pxe, container registry. . Note Services like ironic, pxe, keepalived, httpd and the container registry are running in the host as containers attached to the host network on the host‚Äôs provisioning interface. On the other hand, the vbmc service is also running as a privileged container and it is listening in the host‚Äôs baremetal interface. Once the network is configured, a local container registry is started. It will be needed in the case of using locally built images. In that case, the container images can be modified locally and pushed to the local registry. At that point, the specific image location variable must be changed so it must point out the local registry. This process makes it easy to verify and test changes to the code locally. At this point, the following containers are running inside two pods on the host: infra-pod and ironic-pod. [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID67cc53713145  infra-pod  Running  6 days ago  [vbmc] [sushy-tools] [httpd-infra] [67cc53713145-infra]  f1da23fcd77f5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407Below are detailed the containers inside the infra-pod pod which are running as privileged using the host network:    The httpd container. &gt; &gt;A folder called shared where the cloud OS image and IPA files are available is mounted and exposed to the virtual bare metal hosts.     sudo podman run -d ‚Äìnet host ‚Äìprivileged ‚Äìname httpd-infra ‚Äìpod infra-pod -v /opt/metal3-dev-env/ironic:/shared ‚Äìentrypoint /bin/runhttpd quay. io/metal3-io/ironic&gt; This folder also contains the `inspector. ipxe` file which contains the information needed to be able to run the `ironic-python-agent` kernel and initramfs. Below, httpd-infra container is accessed and it has been verified that host's `/opt/metal3-dev-env/ironic/` (`IRONIC_DATA_DIR`) is mounted inside the *shared* folder of the container:```sh[alosadag@eko1 metal3-dev-env]$ sudo podman exec -it httpd-infra bash[root@infra-pod shared]# cat html/inspector. ipxe#!ipxe:retry_bootecho In inspector. ipxeimgfree# NOTE(dtantsur): keep inspection kernel params in [mdns]params in ironic-inspector-imagekernel --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. kernel ipa-inspection-callback-url=http://172. 22. 0. 1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd. journald. forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-dhcp-all-interfaces=1 ipa-collect-lldp=1 initrd=ironic-python-agent. initramfs || goto retry_bootinitrd --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. initramfs || goto retry_bootboot   The vbmc container.   This container mounts two host folders: one is /opt/metal3-dev-env/virtualbmc/vbmc where vbmc configuration for each node is stored, the other folder is /root/. ssh where root keys are located, specifically id_rsa_virt_power which is used to manage the communication with libvirt.  + sudo podman run -d --net host --privileged --name vbmc --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/&gt; . vbmc -v /root/. ssh:/root/ssh quay. io/metal3-io/vbmc    The sushy-tools container.   This container mounts the /opt/metal3-dev-env/virtualbmc/sushy-tools config folder and the/root/. sshlocal folder as well. The functionality is similar as thevbmc`, however this use redfish instead of ipmi to connect to the BMC.  + sudo podman run -d --net host --privileged --name sushy-tools --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/&gt; sushy-tools:/root/sushy -v /root/. ssh:/root/ssh quay. io/metal3-io/sushy-tools Information At this point the virtual infrastructure must be ready to apply the Kubernetes specific configuration. Note that all the VMs specified by NUMBER_NODES and minikube must be shut down and the defined virtual network must be active: [alosadag@smc-master metal3-dev-env]$ sudo virsh list --all Id  Name              State---------------------------------------------------- -   minikube            shut off -   node_0             shut off -   node_1             shut off -   node_2             shut off[alosadag@smc-master metal3-dev-env]$ sudo virsh net-list --all Name         State   Autostart   Persistent---------------------------------------------------------- baremetal      active   yes      yes default       active   yes      yes minikube-net     active   yes      yes provisioning     active   yes      yesIn the video below it is exhibited all the configuration explained and executed during this second step. Step 3: Launch the management cluster (minikube): The third script called 03_launch_mgmt_cluster. sh basically configures minikube to become a Metal¬≥ management cluster. On top of minikube the baremetal-operator, capi-controller-manager, capbm-controller-manager and cabpk-controller-manager are installed in the metal3 namespace. In a more detailed way, the script clones the Bare Metal Operator (BMO) and Cluster API Provider for Managed Bare Metal Hardware operator (CAPBM) git repositories, creates the cloud. yaml file and starts the minikube virtual machine. Once minikube is up and running, the BMO is built and executed in minikube‚Äôs Kubernetes cluster. In the case of the Bare Metal Operator, the branch by default to clone is master, however, this and other variables shown in the following table can be replaced in the config file: + BMOREPO=https://github. com/metal3-io/baremetal-operator. git+ BMOBRANCH=master      Name of the variable   Default value   Options         BMOREPO   https://github. com/metal3-io/baremetal-operator. git   ¬†       BMOBRANCH   master   ¬†       CAPBMREPO   https://github. com/metal3-io/cluster-api-provider-baremetal. git   ¬†       CAPM3_VERSION   v1alpha3   v1alpha4 or v1alpha3       FORCE_REPO_UPDATE   false   ¬†       BMO_RUN_LOCAL   false   ¬†       CAPBM_RUN_LOCAL   false   ¬†   Once the BMO variables are configured, it is time for the operator to be deployed using kustomize and kubectl as it can seen from the logs:  Information: Kustomize is a Kubernetes tool that lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. + kustomize build bmo-dirPrHIrcl+ kubectl apply -f-namespace/metal3 createdcustomresourcedefinition. apiextensions. k8s. io/baremetalhosts. metal3. io createdserviceaccount/metal3-baremetal-operator createdclusterrole. rbac. authorization. k8s. io/metal3-baremetal-operator createdclusterrolebinding. rbac. authorization. k8s. io/metal3-baremetal-operator createdconfigmap/ironic-bmo-configmap-75tkt49k5c createdsecret/mariadb-password-d88m524c46 createddeployment. apps/metal3-baremetal-operator createdOnce the BMO objects are applied, it‚Äôs time to transform the virtual bare metal hosts information into a yaml file of kind BareMetalHost Custom Resource (CR). This is done by a golang script passing them the IPMI address, BMC username and password, which are stored as a Kubernetes secret, MAC address and name: + go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6230 -password password -user admin -boot-mac 00:be:bc:fd:17:f3 node-0+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6231 -password password -user admin -boot-mac 00:be:bc:fd:17:f7 node-1+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6232 -password password -user admin -boot-mac 00:be:bc:fd:17:fb node-2+ read -r name address user password macBelow is shown the bare metal host definition of node-1. Note that the IPMI address is the IP of the host‚Äôs provisioning interface. Behind the scenes, IPMI is handled by the vbmc container running in the host. ---apiVersion: v1kind: Secretmetadata: name: node-1-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: node-1spec: online: true bootMACAddress: 00:00:e0:4b:24:8f bmc:  address: ipmi://192. 168. 111. 1:6231  credentialsName: node-1-bmc-secretSee that the MAC address configured in the BareMetalHost spec definition matches node-1 provisioning interface: [root@eko1 metal3-dev-env]# virsh domiflist node_1Interface Type    Source   Model    MAC-------------------------------------------------------vnet4   bridge   provisioning virtio   00:00:e0:4b:24:8fvnet5   bridge   baremetal virtio   00:00:e0:4b:24:91Finally, the script apply in namespace metal3 each of the BareMetalHost yaml files that match each virtual bare metal host: + kubectl apply -f bmhosts_crs. yaml -n metal3secret/node-0-bmc-secret createdbaremetalhost. metal3. io/node-0 createdsecret/node-1-bmc-secret createdbaremetalhost. metal3. io/node-1 createdsecret/node-2-bmc-secret createdbaremetalhost. metal3. io/node-2 createdLastly, it is the turn of the CAPBM. Similar to BMO, kustomize is used to create the different Kubernetes components and kubectl applied the files into the management cluster. Warning Note that installing CAPBM includes installing the components of the Cluster API and the components of the Cluster API bootstrap provider kubeadm (CABPK) Below the objects are created through the generate. sh script: ++ mktemp -d capbm-XXXXXXXXXX+ kustomize_overlay_path=capbm-eJPOjCPASD+ . /examples/generate. sh -fGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/cluster. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/controlplane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3crds. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3plane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/machinedeployment. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-cluster-api. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-kubeadm. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-baremetal. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/provider-components. yamlThen, kustomize configures the files accordingly to the values defined and kubectl applies them: + kustomize build capbm-eJPOjCPASD+ kubectl apply -f-namespace/cabpk-system creatednamespace/capbm-system creatednamespace/capi-system createdcustomresourcedefinition. apiextensions. k8s. io/baremetalclusters. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachines. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachinetemplates. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/clusters. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigs. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigtemplates. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinedeployments. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machines. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinesets. cluster. x-k8s. io createdrole. rbac. authorization. k8s. io/cabpk-leader-election-role createdrole. rbac. authorization. k8s. io/capbm-leader-election-role createdrole. rbac. authorization. k8s. io/capi-leader-election-role createdclusterrole. rbac. authorization. k8s. io/cabpk-manager-role createdclusterrole. rbac. authorization. k8s. io/cabpk-proxy-role createdclusterrole. rbac. authorization. k8s. io/capbm-manager-role createdclusterrole. rbac. authorization. k8s. io/capbm-proxy-role createdclusterrole. rbac. authorization. k8s. io/capi-manager-role createdrolebinding. rbac. authorization. k8s. io/cabpk-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capbm-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capi-leader-election-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capi-manager-rolebinding createdsecret/capbm-webhook-server-secret createdservice/cabpk-controller-manager-metrics-service createdservice/capbm-controller-manager-service createdservice/capbm-controller-metrics-service createddeployment. apps/cabpk-controller-manager createddeployment. apps/capbm-controller-manager createddeployment. apps/capi-controller-manager createdInformation At this point all controllers and operators must be running in the namespace metal3 of the management cluster (minikube). All virtual bare metal hosts configured must be shown as BareMetalHosts resources in the metal3 namespace as well. They should be in ready status and stopped (online is false) In the video below it is exhibited all the configuration explained and executed during this third step. Step 4: Verification: The last script 04_verify. sh is in charge of verifying that the deployment has been successful by checking several things:  Custom resources (CR) and custom resource definition (CRD) were applied and exist in the cluster.  Verify that the virtual bare metal hosts matche the information detailed in theBareMetalHost object.  All containers are in running status.  Verify virtual network configuration and status.  Verify operators and controllers are running. However, this verification can be easily achieved manually. For instance, checking that controllers and operators running in the management cluster (minikube) and all the virtual bare metal hosts are in ready status: [alosadag@eko1 ~]$ kubectl get pods -n metal3 -o wideNAME                     READY  STATUS  RESTARTS  AGE   IP        NODE    NOMINATED NODE  READINESS GATEScabpk-controller-manager-5c67dd56c4-wfwbh  2/2   Running  9     6d23h  172. 17. 0. 5    minikube  &lt;none&gt;      &lt;none&gt;capbm-controller-manager-7f9b8f96b7-grl4r  2/2   Running  12     6d23h  172. 17. 0. 4    minikube  &lt;none&gt;      &lt;none&gt;capi-controller-manager-798c76675f-dxh2n   1/1   Running  10     6d23h  172. 17. 0. 6    minikube  &lt;none&gt;      &lt;none&gt;metal3-baremetal-operator-5b4c59755d-h4zkp  6/6   Running  8     6d23h  192. 168. 39. 101  minikube  &lt;none&gt;      &lt;none&gt;Verify that the BareMetalHosts provisioning status is ready and the BMC configuration is correct. Check that all virtual bare metal hosts are shut down (online is false): [alosadag@eko1 ~]$ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER       BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    ready                   ipmi://192. 168. 111. 1:6230  unknown      falsenode-1  OK    ready                   ipmi://192. 168. 111. 1:6231  unknown      falsenode-2  OK    ready                   ipmi://192. 168. 111. 1:6232  unknown      falseGet the list of CRDs created in the cluster. Check that, at least, the following ones exist: [alosadag@eko1 ~]$ kubectl get crdsNAME                            CREATED ATbaremetalclusters. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalhosts. metal3. io                  2020-01-22T13:19:35Zbaremetalmachines. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalmachinetemplates. infrastructure. cluster. x-k8s. io  2020-01-22T13:19:42Zclusters. cluster. x-k8s. io                  2020-01-22T13:19:42Zkubeadmconfigs. bootstrap. cluster. x-k8s. io          2020-01-22T13:19:42Zkubeadmconfigtemplates. bootstrap. cluster. x-k8s. io      2020-01-22T13:19:42Zmachinedeployments. cluster. x-k8s. io             2020-01-22T13:19:43Zmachines. cluster. x-k8s. io                  2020-01-22T13:19:43Zmachinesets. cluster. x-k8s. io                2020-01-22T13:19:43ZInformation KUBECONFIG file is stored in the user‚Äôs home directory (~/. kube/config) that executed the scripts. Check the status of all the applications running in minikube or better said, in the management cluster. [alosadag@smc-master logs]$ kubectl get pods -ANAMESPACE   NAME                    READY  STATUS  RESTARTS  AGEkube-system  coredns-6955765f44-fkdzp          1/1   Running  1     164mkube-system  coredns-6955765f44-fxzvz          1/1   Running  1     164mkube-system  etcd-minikube                1/1   Running  1     164mkube-system  kube-addon-manager-minikube         1/1   Running  1     164mkube-system  kube-apiserver-minikube           1/1   Running  1     164mkube-system  kube-controller-manager-minikube      1/1   Running  1     164mkube-system  kube-proxy-87g98              1/1   Running  1     164mkube-system  kube-scheduler-minikube           1/1   Running  1     164mkube-system  storage-provisioner             1/1   Running  2     164mmetal3    cabpk-controller-manager-5c67dd56c4-rldk4  2/2   Running  0     156mmetal3    capbm-controller-manager-7f9b8f96b7-mdfcw  2/2   Running  0     156mmetal3    capi-controller-manager-84947c7497-k6twl  1/1   Running  0     156mmetal3    metal3-baremetal-operator-78bffc8d-z5hqs  6/6   Running  0     156mIn the video below it is exhibited all the configuration explained and executed during the verification steps. Summary: In this post a deep dive into the metal3-dev-env scripts was shown. It has been deeply detailed the process of creating a Metal¬≥ emulated environment from a set of virtual machines (VMs) to manage as if they were bare metal hosts. After this post, the reader should have acquired a basic understanding of all the pieces involved in the Metal¬≥ project. Also, and more important, how these scripts can be adapted to your specific needs. Remember that this can be achieved in multiple ways: replacing values in the global variables, replacing Ansible default variables or even modifying playbooks or the scripts themselves. Notice that the Metal¬≥ development environment also focuses on developing new features of the BMO or CAPBM and being able to test them locally. References:  Video playlist: A detailed walkthrough the installation of the metal3-dev-env on Youtube Getting started with Metal3. io Metal¬≥ code repositories"
    }, {
    "id": 8,
    "url": "/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html",
    "title": "Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, shiftdev, edge",
    "body": "Conference talk: Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat: Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk at the Shift Dev 2019 - Developer Conference about the most cutting-edge technologies, techniques and biggest trends in the developer space. In this video, Yolanda Robla speaks about the deployment of Kubernetes on Bare Metal with the help of Metal¬≥, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Yolanda Robla Yolanda Robla is a Principal Software Engineer at Red Hat. In her own words:  In my current position in Red Hat as an NFV Partner Engineer, I investigate new technologies and create proofs of concept for partners to embrace new technologies. Being the current PTL of Akraino, I am involved in designing and implementing systems based on Kubernetes for the Edge use cases, ensuring high scalability and reproducibility using a GitOps approach. References:  Video: Metal¬≥: Deploy Kubernetes on Bare Metal video"
    }, {
    "id": 9,
    "url": "/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html",
    "title": "Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann, Red Hat - KubeCon NA, November 2019",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, kubecon, edge",
    "body": "Conference talk: Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat: Metal¬≥ (metal cubed/Kube) is a new open-source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. Metal¬≥ enables the management of bare metal hosts via custom resources managed through the Kubernetes API as well as the monitoring of bare metal host metrics to Prometheus. This presentation will explain the motivations behind creating the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using Metal¬≥ in a Kubernetes cluster. In this video, Russell Bryant and Doug Hellmann speak about the what‚Äôs and how‚Äôs of Metal¬≥, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Russell Bryant Russell Bryant is a Distinguished Engineer at Red Hat, where he works on infrastructure management to support Kubernetes clusters. Prior to working on the Metal¬≥ project, Russell worked on other open infrastructure projects. Russell worked in Software Defined Networking with Open vSwitch (OVS) and Open Virtual Network (OVN) and worked on various parts of OpenStack. Russell also worked in open source telephony via the Asterisk project. Doug Hellmann Doug Hellmann is a Senior Principal Software Engineer at Red Hat. He has been a professional developer since the mid-1990s and has worked on a variety of projects in fields such as mapping, medical news publishing, banking, data centre automation, and hardware provisioning. He has been contributing to open-source projects for most of his career and for the past 7 years he has been focusing on open-source cloud computing technologies, including OpenStack and Kubernetes. References:  Presentation: Introducing Metal¬≥ KubeCon NA 2019 PDF Video: Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management videoDemos:  First demo (Inspection)  Second demo (Provisioning)  Third demo (Scale up)  Fourth demo (v1alpha2) "
    }, {
    "id": 10,
    "url": "/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html",
    "title": "Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, summit, edge",
    "body": "Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma: A critical part of being successful in the hybrid cloud is being successful in your data centre with your own infrastructure. In this video, Paul Cormier, Burr Sutter and Garima Sharma show how you can bring the Open Hybrid cloud to the edge. Cluster management from multiple cloud providers to on-premise. In the demo you‚Äôll see a multi-cluster inventory for the open hybrid cloud at cloud. redhat. com, OpenShift Container Storage providing storage for Virtual Machines and containers (Cloud, Virtualization and bare metal), and everything Kubernetes native. Speakers: Paul Cormier Executive vice president and president, Products and Technologies. Leads Red Hat‚Äôs technology and products organizations, including engineering, product management, and product marketing for Red Hat‚Äôs technologies. He joined Red Hat in May 2001 as executive vice president, Engineering. Burr Sutter A lifelong developer advocate, community organizer, and technology evangelist, Burr Sutter is a featured speaker at technology events around the globe ‚Äîfrom Bangalore to Brussels and Berlin to Beijing (and most parts in between)‚Äî he is currently Director of Developer Experience at Red Hat. A Java Champion since 2005 and former president of the Atlanta Java User Group, Burr founded the DevNexus conference ‚Äînow the second largest Java event in the U. S. ‚Äî with the aim of making access to the world‚Äôs leading developers affordable to the developer community. Garima Sharma Senior Engineering leader at the world‚Äôs largest Open Source company. As a seasoned Tech professional, she runs a global team of Solutions Engineers focused on a large portfolio of Cloud Computing products and technology. She has helped shape science and technology for mission-critical software, reliability in operations and re-design of architecture all geared towards advancements in medicine, security, cloud technologies and bottom-line savings for the client businesses. Whether leading the architecture, development and delivery of customer-centric cutting-edge systems or spearheading diversity and inclusion initiatives via keynotes, blogs and conference presentations, Garima champions the idea of STEM. Garima ardently believes in Maya Angelou‚Äôs message that diversity makes for a rich tapestry, and we must understand that all the threads of the tapestry are equal in value no matter what their colour. Video:  Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019"
    }, {
    "id": 11,
    "url": "/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html",
    "title": "Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack, ironic",
    "body": "Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat: In this session, you can hear about a new effort to enable baremetal Kubernetes deployments using native interfaces, and in particular, the Kubernetes Operator framework, combined with OpenStack Ironic. This approach aims to seamlessly integrate your infrastructure with your workloads, including baremetal servers, storage and container/VM workloads. All this can be achieved using kubernetes native applications, combined with existing, proven deployment and storage tooling. In this talk, we cover the options around Kubernetes deployments today, the specific approach taken by the new Kubernetes-native ‚ÄúMetalKube‚Äù project, and the status/roadmap of this new community effort. Speakers: Steve Hardy is a Senior Principal Software Engineer at Red Hat, currently involved in kubernetes/OpenShift deployment and architecture. He is also an active member of the OpenStack community and has been a project team leader of both the Heat (orchestration) and TripleO (deployment) projects. References:  Open Infrastructure Days UK 2019, Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat"
    }, {
    "id": 12,
    "url": "/blog/2019/10/31/OpenStack-Ironic-and-Bare-Metal-Infrastructure_All-Abstractions-Start-Somewhere.html",
    "title": "OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack",
    "body": "Conference talk: OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere: The history of cloud computing has rapidly layered abstractions on abstractions to deliver applications faster, more reliably, and easier. Serverless functions on top of containers on top of virtualization. However, at the bottom of every stack is physical hardware that has an entire lifecycle that needs to be managed. In this video, Chris and Julia show how OpenStack Ironic is a solution to the problem of managing bare-metal infrastructure. Speakers: Chris Hoge is a Senior Strategic Program Manager for the OpenStack foundation. He‚Äôs been an active contributor to the Interop Working Group (formerly DefCore) and helps run the trademark program for the OpenStack Foundation. He also works on collaborations between the OpenStack and Kubernetes communities. Previously he worked as an OpenStack community manager and developer at Puppet Labs and operated a research cloud for the College of Arts and Sciences at The University of Oregon. When not cloud computing, he enjoys long-distance running, dancing, and throwing a ball for his Border Collie. Julia Kreger is Principal Software Engineer at Red Hat. She started her career in networking and eventually shifted to systems engineering. The DevOps movement leads her into software development and the operationalization of software due to the need to automate large-scale systems deployments. She is experienced in conveying an operational perspective while bridging that with requirements and doesn‚Äôt mind getting deep down into code to solve a problem. She is an active core contributor and leader in the OpenStack Ironic project, which is a project she feels passionate about due to many misspent hours in data centres deploying hardware. Prior to OpenStack, Julia contributed to the Shared Learning Infrastructure and worked with large-scale litigation database systems. References:  Open Infrastructure Summit, Denver, CO, April 29 - May 1, 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere"
    }, {
    "id": 13,
    "url": "/blog/2019/09/11/Baremetal-operator.html",
    "title": "Baremetal Operator",
    "author" : "Pablo Iranzo G√≥mez",
    "tags" : "openshift, kubernetes, metal3, operator",
    "body": "Introduction: The baremetal operator, documented at https://github. com/metal3-io/baremetal-operator/blob/master/docs/api. md, it‚Äôs the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances. Quoting from the project:  The Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:Inspect the host‚Äôs hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more. Provision hosts with a desired imageClean a host‚Äôs disk contents before or after provisioning. A bit more in deep approach: The Baremetal Operator (BMO) keeps a mapping of each host and its management interfaces (vendor-based like iLO, iDrac, iRMC, etc) and is controlled via IPMI. All of this is defined in a CRD, for example: apiVersion: v1kind: Secretmetadata: name: metal3-node01-credentials namespace: metal3type: Opaquedata: username: YWRtaW4= password: YWRtaW4=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: metal3-node01 namespace: metal3spec: bmc:  address: ipmi://172. 22. 0. 2:6230  credentialsName: metal3-node01-credentials bootMACAddress: 00:c2:fc:3b:e1:01 description:    hardwareProfile:  libvirt  online: falseWith above values (described in API), we‚Äôre telling the operator:  MAC: Defines the mac address of the NIC connected to the network that will be used for the provisioning of the host bmc: defines the management controller address and the secret used credentialsName: Defines the name of the secret containing username/password for accessing the IPMI serviceOnce the server is ‚Äòdefined‚Äô via the CRD, the underlying service (provided by ironic1 as of this writing) is inspected: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    inspecting            ipmi://172. 22. 0. 1:6230           falseOnce the inspection has finished, the status will change to ready and made available for provisioning. When we define a machine, we refer to the images that will be used for the actual provisioning in the CRD (image): apiVersion: v1data: userData: DATAkind: Secretmetadata: name: metal3-node01-user-data namespace: metal3type: Opaque---apiVersion:  cluster. k8s. io/v1alpha1 kind: Machinemetadata: name: metal3-node01 namespace: metal3 generateName: baremetal-machine-spec: providerSpec:  value:   apiVersion:  baremetal. cluster. k8s. io/v1alpha1    kind:  BareMetalMachineProviderSpec    image:    url: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2    checksum: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum   userData:    name: metal3-node01-user-data    namespace: metal3[root@metal3-kubernetes ~]# kubectl create -f metal3-node01-machine. ymlsecret/metal3-node01-user-data createdmachine. cluster. k8s. io/metal3-node01 createdLet‚Äôs examine the annotation created when provisioning (metal3. io/BareMetalHost): [root@metal3-kubernetes ~]# kubectl get machine -n metal3 metal3-node01 -o yamlapiVersion: cluster. k8s. io/v1alpha1kind: Machinemetadata: annotations:  metal3. io/BareMetalHost: metal3/metal3-node01 creationTimestamp:  2019-07-08T15:30:44Z  finalizers: - machine. cluster. k8s. io generateName: baremetal-machine- generation: 2 name: metal3-node01 namespace: metal3 resourceVersion:  6222  selfLink: /apis/cluster. k8s. io/v1alpha1/namespaces/metal3/machines/metal3-node01 uid: 1bfd384a-5467-43b7-98aa-e80e1ace5ce7spec: metadata:  creationTimestamp: null providerSpec:  value:   apiVersion: baremetal. cluster. k8s. io/v1alpha1   image:    checksum: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum    url: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2   kind: BareMetalMachineProviderSpec   userData:    name: metal3-node01-user-data    namespace: metal3 versions:  kubelet:   status: addresses: - address: 192. 168. 122. 79  type: InternalIP - address: 172. 22. 0. 39  type: InternalIP - address: localhost. localdomain  type: Hostname lastUpdated:  2019-07-08T15:30:44Z In the output above, the host assigned was the one we‚Äôve defined earlier as well as the other parameters like IP‚Äôs, etc generated. Now, if we check baremetal hosts, we can see how it‚Äôs getting provisioned: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    provisioned            ipmi://172. 22. 0. 1:6230           trueAnd also, check it via the ironic command: [root@metal3-kubernetes ~]# export OS_TOKEN=fake-token ; export OS_URL=http://localhost:6385 ; openstack baremetal node list+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| UUID                 | Name     | Instance UUID            | Power State | Provisioning State | Maintenance |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| 7551cfb4-d758-4ad8-9188-859ee53cf298 | metal3-node01 | 7551cfb4-d758-4ad8-9188-859ee53cf298 | power on  | active       | False    |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+Wrap-up: We‚Äôve seen how via a CRD we‚Äôve defined credentials for a baremetal host to make it available to get provisioned and how we‚Äôve also defined a machine that was provisioned on top of that baremetal host.       Ironic was chosen as the initial provider for baremetal provisioning, check Ironic documentation for more details about Ironic usage in Metal¬≥¬†&#8617;    "
    }, {
    "id": 14,
    "url": "/blog/2019/06/25/Metal3.html",
    "title": "Metal3",
    "author" : "Eduardo Minguez",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://www. underkube. com/posts/2019-06-25-metal3/ In this blog post, I‚Äôm going to try to explain in my own words a high leveloverview of what Metal3 is, the motivation behind it and some concepts relatedto a ‚Äòbaremetal operator‚Äô. Let‚Äôs have some definitions! Custom Resource Definition: The k8s API provides out-of-the-box objects such as pods, services, etc. There are a few methods of extending the k8s API (such as API extensions)but since a few releases back, the k8s API can be extended easily with custom resources definitions (CRDs). Basically, this means you can virtually create any type of object definition in k8s(actually only users with cluster-admin capabilities) with a yaml such as: apiVersion: apiextensions. k8s. io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;. &lt;group&gt; name: crontabs. stable. example. comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable. example. com # list of versions supported by this CustomResourceDefinition versions:  - name: v1   # Each version can be enabled/disabled by Served flag.    served: true   # One and only one version must be marked as the storage version.    storage: true # either Namespaced or Cluster scope: Namespaced names:  # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;  plural: crontabs  # singular name to be used as an alias on the CLI and for display  singular: crontab  # kind is normally the CamelCased singular type. Your resource manifests use this.   kind: CronTab  # shortNames allow shorter string to match your resource on the CLI  shortNames:   - ct preserveUnknownFields: false validation:  openAPIV3Schema:   type: object   properties:    spec:     type: object     properties:      cronSpec:       type: string      image:       type: string      replicas:       type: integerAnd after kubectl apply -f you can kubectl get crontabs. There is a ton of information with regards to CRDs, like the k8s official documentation. The CRD by himself is not useful per se as nobody will take care of it (that‚Äôs why I said definition). Itrequires a controller to watch for those new objects and react to differentevents affecting the object. Controller: A controller is basically a loop that watches the current status of an objectand if it is different from the desired status, it fixes it (reconciliation). This is why k8s is ‚Äòdeclarative‚Äô, you specify the object desired status instead‚Äòhow to do it‚Äô (imperative). Again, there are tons of documentation (and examples) around the controller pattern which isbasically the k8s roots, so I‚Äôll let your google-foo take care of it :) Operator: An Operator (in k8s slang) is an application running in your k8scluster that deploys, manages and maintains (so, operates) a k8s application. This k8s application (the one that the operator manages), can be as simple as a ‚Äòhello world‚Äô applicationcontainerized and deployed in your k8s cluster or it can be a much more complexthing, such as a database cluster. The ‚Äòoperator‚Äô is like an ‚Äòexpert sysadmin‚Äô containerized that takes care ofyour application. Bear in mind that the ‚Äòexpert‚Äô tag (meaning the automation behind the operator)depends on the operator implementation‚Ä¶ so there can be basic operators thatonly deploy your application or complex operators that handle day 2 operationssuch as upgrades, failovers, backup/restore, etc. See the CoreOS operator definition for more information. Cloud Controller Manager: k8s code is smart enough to be able to leveragethe underlying infrastructure where the cluster is running, such as being ableof creating ‚ÄòLoadBalancer‚Äô services, understanding the cluster topology based on the cloud provider AZs where the nodes are running (for scheduling reasons), etc. This task of ‚Äòtalking to the cloud provider‚Äô is performed by the Cloud Controller Manager (CCM) and for moreinformation, you can take a look at the official k8s documentation withregards the architecture and the administration (also, if you are brave enough, you can create your own cloud controller manager ) Cluster API: The Cluster API implementation is a WIP ‚Äòframework‚Äô that allows a k8s cluster to manage itself, including the ability to create new clusters, add more nodes, etc. in a ‚Äòk8s way‚Äô (declarative, controllers, CRDs, etc. ), so there are objects such as Cluster that can be expressed as k8s objects: apiVersion:  cluster. k8s. io/v1alpha1 kind: Clustermetadata: name: myclusterspec: clusterNetwork:  services:   cidrBlocks: [ 10. 96. 0. 0/12 ]  pods:   cidrBlocks: [ 192. 168. 0. 0/16 ]  serviceDomain:  cluster. local  providerSpec: . . . but also:  Machine type objects [MachineSet type objects](https://github. com/kubernetes-sigs/cluster-api/blob/master/pkg/apis/cluster/v1alpha1/machineset_types. go) MachineDeployment type objects etcThere are someprovider implementations in the wild such as AWS, Azure, GCP, OpenStack,vSphere, etc. ones and the Cluster API project is driven by the SIG Cluster Lifecycle. Please review the official Cluster API repository for more information. Actuator: The actuator is a Cluster API interface that reacts to changes to Machineobjects reconciliating the Machine status. The actuator code is tightly coupled with the provider (that‚Äôs why it is aninterface) such as the AWS one. MachineSet vs Machine: To simplify, let‚Äôs say that MachineSets are to Machines what ReplicaSets areto Pods. So you can scale the Machines in your cluster just by changingthe number of replicas of a MachineSet. Cluster API vs Cloud Providers: As we have seen, the Cluster API leverages the provider related to the k8sinfrastructure itself (clusters and nodes) and the CCM and the cloud providerintegration for k8s is to leverage the cloud provider to provide support infrastructure. Let‚Äôs say Cluster API is for the k8s administrators and theCCM is for the k8s users :) Machine API: The OpenShift 4 Machine API is a combination of some of the upstream Cluster APIwith custom OpenShift resources and it is designed to work in conjunction withthe Cluster Version Operator. OpenShift‚Äôs Machine API Operator: The machine-api-operator isan operator that manages the Machine API objects in an OpenShift 4 cluster. The operator is capable of creating machines in AWS and libvirt (more providerscoming soon) via the Machine Controller and it is included out of thebox with OCP 4 (and can be deployed in a k8s vanilla as well) Baremetal: A baremetal server (or bare-metal) is just a computer server. The last year‚Äôs terms such as virtualization, containers, serverless, etc. have beenpopular but at the end of the day, all the code running on top of a SaaS, PaaSor IaaS is actually running in a real physical server stored in a datacenterwired to routers, switches and power. That server is a ‚Äòbaremetal‚Äô server. If you are used to cloud providers and instances, you probably don‚Äôt know thepains of baremetal management‚Ä¶ including things such as connecting to thevirtual console (usually it requires an old Java version) to debug issues,configuring pxe for provisioning baremetal hosts (or attach ISOs via the virtual console‚Ä¶ or insert a CD/DVD physically into the CD carry if you are‚Äòlucky‚Äô enough‚Ä¶), configuring VLANs for traffic isolation, etc. That kind of operation is not ‚Äòcloud‚Äô ready and there are tools that providebaremetal management, such as maas or ironic. Ironic: OpenStack bare metal provisioning (or ironic) is an open source project (or even better, a number of open source projects) to manage baremetal hosts. Ironic avoids the administrator dealing with pxe configuration, manual deployments, etc. and provides a defined API and a series of plugins to interact with different baremetal models and vendors. Ironic is used in OpenStack to provide baremetal objects but there are someprojects (such as bifrost) to useIronic ‚Äòstandalone‚Äô (so, no OpenStack required) Metal3: Metal3 is a project aimed at providing a baremetal operator thatimplements the Cluster API framework required to be able to manage baremetalin a k8s way (easy peasy!). It uses ironic under the hood to avoid reinventing thewheel, but consider it as an implementation detail that may change. The Metal3 baremetal operator watches for BareMetalHost (CRD) objects defined as: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-worker-0spec: online: true bootMACAddress: 00:11:22:33:44:55 bmc:  address: ipmi://my-worker-0. ipmi. example. com  credentialsName: my-worker-0-bmc-secretThere are a few more fields in the BareMetalHost object such as the image, hardware profile, etc. The Metal3 project is actually divided into two different components: baremetal-operator: The Metal3 baremetal-operator is the component that manages baremetal hosts. It exposes a new BareMetalHost custom resource in the k8s API that lets you manage hosts in a declarative way. cluster-api-provider-baremetal: The Metal3 cluster-api-provider-baremetal includes the integration with the Cluster API project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. BareMetalHost vs Machine vs Node:  BareMetalHost is a Metal3 object Machine is a Cluster API object Node is where the pods run :)Those three concepts are linked in a 1:1:1 relationship meaning: A BareMetalHost created with Metal3 maps to a Machine object and once theinstallation procedure finishes, a new kubernetes node will be added to thecluster. $ kubectl get nodesNAME                     STATUS  ROLES  AGE  VERSIONmy-node-0. example. com            Ready  master  25h  v1. 14. 0$ kubectl get machines --all-namespacesNAMESPACE        NAME         INSTANCE  STATE  TYPE  REGION  ZONE  AGEopenshift-machine-api  my-node-0                          25h$ kubectl get baremetalhosts --allnamespacesNAMESPACE       NAME   STATUS PROVISIONING STATUS MACHINE BMC HARDWARE PROFILE ONLINE ERRORopenshift-machine-api my-node-0 OK   provisioned my-node-0. example. com ipmi://1. 2. 3. 4 unknown trueThe 1:1 relationship for the BareMetalHost and the Machine is stored in themachineRef field in the BareMetalHost object: $ kubectl get baremetalhost/my-node-0 -n openshift-machine-api -o jsonpath='{. spec. machineRef}'map[name:my-node-0 namespace:openshift-machine-api]In a Machine annotation: $ kubectl get machines my-node-0 -n openshift-machine-api -o jsonpath='{. metadata. annotations}'map[metal3. io/BareMetalHost:openshift-machine-api/my-node-0]The node reference is stored in the . status. nodeRef. name field in theMachine object: $ kubectl get machine my-node-0 -o jsonpath='{. status. nodeRef. name}'my-node-0. example. comRecap: Being able to ‚Äòjust scale a node‚Äô in k8s means a lot of underlying concepts and technologies involved behind the scenes :) Resources/links:  https://dzone. com/articles/introducing-the-kubernetes-cluster-api-project-2 https://tanzu. vmware. com/content/blog/the-what-and-the-why-of-the-cluster-api https://github. com/kubernetes-sigs/cluster-api https://github. com/kubernetes-sigs/cluster-api-provider-aws https://itnext. io/deep-dive-to-cluster-api-a0b4e792d57d https://www. linux. com/blog/event/kubecon/2018/4/extending-kubernetes-cluster-api"
    }, {
    "id": 15,
    "url": "/blog/2019/05/13/The_new_stack_Metal3_Uses_OpenStack_Ironic_for_Declarative_Bare_Metal_Kubernetes.html",
    "title": "The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, OpenStack, ironic",
    "body": "The new stack Metal¬≥ Uses OpenStack‚Äôs Ironic for Declarative Bare Metal Kubernetes: Mike Melanson talks in this article about the Open Infrastructure Summit in Denver, Colorado. Where bare metal was one of the main leads of the event. During this event, the OpenStack Foundation unveil a new project called Metal¬≥ (pronounced ‚Äúmetal cubed‚Äù) that uses Ironic ‚Äúas a foundation for declarative management of bare metal infrastructure for Kubernetes‚Äù. He also comments on how James Penick, Chris Hoge, senior strategic program manager at OpenStack Foundation,and Julia Kreger, OpenStack Ironic Project Team Leader, took to the stage to offer a demonstration of Metal3,the new project that provides ‚Äúbare metal host provisioning integration for Kubernetes. ‚Äù Some words from Kreger in an interview with The New Stack:  ‚ÄúI think the bigger trend that we‚Äôre starting to see is a recognition that common tooling and substrate helps everyone succeed faster with more efficiency. ‚Äù  ‚ÄúThis is combined with a shift in the way operators are choosing to solve their problems at scale, specifically in regards to isolation, cost, or performance. ‚Äù For further detail, check out the video of the keynote, which includes a demonstration of Metal3 being used to quickly provision three bare metal servers with Kubernetesor check the full article included below. References:  The new stack: Metal¬≥ Uses OpenStack‚Äôs Ironic for Declarative Bare Metal Kubernetes Video of the keynote: OpenStack Ironic and Baremetal Infrastructure. All Abstractions start somewhere"
    }, {
    "id": 16,
    "url": "/blog/2019/04/30/Metal-Kubed-Baremetal-Provisioning-for-Kubernetes.html",
    "title": "Metal¬≥: Baremetal Provisioning for Kubernetes",
    "author" : "Russell Bryant",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://blog. russellbryant. net/2019/04/30/metal%c2%b3-metal-kubed-bare-metal-provisioning-for-kubernetes/ Project Introduction: There are a number of great open-source tools for bare metal host provisioning, including Ironic. Metal¬≥ aims to build on these technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes. We believe that Kubernetes Native Infrastructure, or managing your infrastructure just like your applications, is a powerful next step in the evolution of infrastructure management. The Metal¬≥ project is also building integration with the Kubernetes cluster-api project, allowing Metal¬≥ to be used as an infrastructure backend for Machine objects from the Cluster API. Metal3 Repository Overview: There is a Metal¬≥ overview and some more detailed design documents in the metal3-docs repository. The baremetal-operator is the component that manages bare metal hosts. It exposes a new BareMetalHost custom resource in the Kubernetes API that lets you manage hosts in a declarative way. Finally, the cluster-api-provider-baremetal repository includes integration with the cluster-api project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. Demo: The project has been going on for a few months now, and there‚Äôs enough now to show some working code. For this demonstration, I‚Äôve started with a 3-node Kubernetes cluster installed using OpenShift. $ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dMachine objects were created to reflect these 3 masters, as well. $ kubectl get machinesNAME       INSTANCE  STATE  TYPE  REGION  ZONE  AGEostest-master-0                       24hostest-master-1                       24hostest-master-2                       24hFor this cluster-api provider, a Machine has a corresponding BareMetalHost object, which corresponds to the piece of hardware we are managing. There is a design document that covers the relationship between Nodes, Machines, and BareMetalHosts. Since these hosts were provisioned earlier, they are in a special externally provisioned state, indicating that we enrolled them in management while they were already running in a desired state. If changes are needed going forward, the baremetal-operator will be able to automate them. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueNow suppose we‚Äôd like to expand this cluster by adding another bare metal host to serve as a worker node. First, we need to create a new BareMetalHost object that adds this new host to the inventory of hosts managed by the baremetal-operator. Here‚Äôs the YAML for the new BareMetalHost: ---apiVersion: v1kind: Secretmetadata: name: openshift-worker-0-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metalkube. org/v1alpha1kind: BareMetalHostmetadata: name: openshift-worker-0spec: online: true bmc:  address: ipmi://192. 168. 111. 1:6233  credentialsName: openshift-worker-0-bmc-secret bootMACAddress: 00:ab:4f:d8:9e:faNow to add the BareMetalHost and its IPMI credentials Secret to the cluster: $ kubectl create -f worker_crs. yamlsecret/openshift-worker-0-bmc-secret createdbaremetalhost. metalkube. org/openshift-worker-0 createdThe list of BareMetalHosts now reflects a new host in the inventory that is ready to be provisioned. It will remain in this ready state until it is claimed by a new Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      trueWe have a MachineSet already created for workers, but it scaled down to 0. $ kubectl get machinesetsNAME       DESIRED  CURRENT  READY  AVAILABLE  AGEostest-worker-0  0     0               24hWe can scale this MachineSet to 1 to indicate that we‚Äôd like a worker provisioned. The baremetal cluster-api provider will then look for an available BareMetalHost, claim it, and trigger provisioning of that host. $ kubectl scale machineset ostest-worker-0 --replicas=1 After the new Machine was created, our cluster-api provider claimed the available host and triggered it to be provisioned. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioning       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      trueThis process takes some time. Under the hood, the baremetal-operator is driving Ironic through a provisioning process. This begins with wiping disks to ensure the host comes up in a clean state. It will eventually write the desired OS image to disk and then reboot into that OS. When complete, a new Kubernetes Node will register with the cluster. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioned       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      true$ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dworker-0  Ready  worker  68s  v1. 13. 4+d4ce02c1dThe following screen cast demonstrates this process, as well: Removing a bare metal host from the cluster is very similar. We just have to scale this MachineSet back down to 0. $ kubectl scale machineset ostest-worker-0 --replicas=0 Once the Machine has been deleted, the baremetal-operator will deprovision the bare metal host. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    deprovisioning               ipmi://192. 168. 111. 1:6233  unknown      falseOnce the deprovisioning process is complete, the bare metal host will be back to its ready state, available in the host inventory to be claimed by a future Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      falseGetting Involved: All development is happening on github. We have a metal3-dev mailing list and use #cluster-api-baremetal on Kubernetes Slack to chat. Occasional project updates are posted to @metal3_io on Twitter. "
    }, {
    "id": 17,
    "url": "/blog/2019/04/12/Raise_some_horns_Red_Hat_s_MetalKube_aims_to_make_Kubernetes_on_bare_machines_simple.html",
    "title": "Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple",
    "author" : "Pedro Ib√°√±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic",
    "body": "The Register; Raise some horns: Red Hat‚Äôs Metal¬≥ aims to make Kubernetes on bare machines simple: Max Smolaks talks in this article about the OpenInfra Days in the UK, 2019: where Metal¬≥ was revealed earlier last week by Steve Hardy, Red Hat‚Äôs senior principal software engineer. The Open Infrastructure Days in the UK is an event organised by the local Open Infrastructure community and supported by the OpenStack Foundation. The Open-source software developers at Red Hat are working on a tool that would simplify the deployment and management of Kubernetes clusters on bare-metal servers. Steve told The Register:  ‚ÄúIn some situations, you won‚Äôt want to run a full OpenStack infrastructure-as-a-service layer to provide, potentially, for multiple Kubernetes clusters‚Äù. Hardy is a notable contributor to OpenStack, having previously worked on Heat and TripleO projects. He said one of the reasons for choosing Ironic was its active development ‚Äì and when new features get added to Ironic, the Metal¬≥ team gets them ‚Äúfor free‚Äù.  ‚ÄúOpenStack has always been a modular set of projects, and people have always had the opportunity to reuse components for different applications. This is just an example of where we are leveraging one particular component for infrastructure management, just as an alternative to using a full infrastructure API,‚Äù Hardy said. Thierry Carrez, veep of engineering at the OpenStack Foundation also told The Register:  ‚ÄúI like the fact that the projects end up being reusable on their own, for the functions they bring to the table ‚Äì this helps us integrate with adjacent communities‚Äù. Hardy also commented:  It‚Äôs still early days for Metal¬≥ - the project has just six contributors, and there‚Äôs no telling when it might reach release. ‚ÄúIt‚Äôs a very, very young project but we are keen to get more community participation and feedback,‚Äù. For further detail, check out the full article at The Register: Raise some horns: Red Hat‚Äôs MetalKube aims to make Kubernetes on bare machines simple. References:  Steve Hardy: Red Hat‚Äôs senior principal software engineer.  Thierry Carrez: veep of engineering at the OpenStack Foundation.  The Register: Raise some horns: Red Hat‚Äôs MetalKube aims to make Kubernetes on bare machines simple"
    }, , {
    "id": 18,
    "url": "/blog/categories.html",
    "title": "Categories",
    "author" : "",
    "tags" : "",
    "body": " -   Blog  Read about the newest updates in the community. &lt;/section&gt;             Categories:        hybrid:               One cluster - multiple providers July 08, 2022                 Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             cloud:               Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             metal3:               One cluster - multiple providers July 08, 2022                 Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020                 Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal¬≥ development environment February 18, 2020                 Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Metal¬≥: Baremetal Provisioning for Kubernetes April 30, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             baremetal:               Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Cluster API provider renaming March 05, 2020                 Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal¬≥ development environment February 18, 2020                 Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             stack:               The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             edge:               One cluster - multiple providers July 08, 2022                 Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openstack:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             ironic:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openshift:               Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal¬≥: Baremetal Provisioning for Kubernetes April 30, 2019             kubernetes:               Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal¬≥: Baremetal Provisioning for Kubernetes April 30, 2019             OpenStack:               The new stack Metal¬≥ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019             operator:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019             summit:               Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019             kubecon:               Introducing Metal¬≥: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019             shiftdev:               Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020             metal3-dev-env:               Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 A detailed walkthrough of the Metal¬≥ development environment February 18, 2020             documentation:               A detailed walkthrough of the Metal¬≥ development environment February 18, 2020             development:               A detailed walkthrough of the Metal¬≥ development environment February 18, 2020             talk:               Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             conference:               Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             meetup:               Metal¬≥: Kubernetes Native Bare Metal Cluster Management - Ma√´l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             cluster API:               One cluster - multiple providers July 08, 2022                 Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020             provider:               One cluster - multiple providers July 08, 2022                 Cluster API provider renaming March 05, 2020             raw image:               Raw image streaming available in Metal3 July 05, 2020             image streaming:               Raw image streaming available in Metal3 July 05, 2020             IPAM:               Introducing the Metal3 IP Address Manager July 06, 2020             ip address manager:               Introducing the Metal3 IP Address Manager July 06, 2020             Pivoting:               Metal3 Introduces Pivoting May 05, 2021             Move:               Metal3 Introduces Pivoting May 05, 2021                     Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move      "
    }, {
    "id": 19,
    "url": "/community-resources.html",
    "title": "Community Resources",
    "author" : "",
    "tags" : "",
    "body": " -     Community Resources    Join conversations with the other people who are involved in the creation, maintenance, and future of Metal3. io. &lt;/section&gt;                Get Connected:                                                                                                                                                                                Mailing List                               Slack                     Twitter                                                                               GitHub                                                                                                       Blog                  Providers of Testing Resources:                     Nordix. Details here.         Bare Metal cloud resources sponsored by packet. net.         Netlify Open Source Plan.         Travis CI, free for open source projects.           "
    }, {
    "id": 20,
    "url": "/documentation.html",
    "title": "Documentation",
    "author" : "",
    "tags" : "",
    "body": " -       Documentation     The Metal¬≥ project (pronounced: Metal Kubed) exists to provide components that allow you to do bare metal host management for Kubernetes. Metal¬≥ works as a Kubernetes application, meaning it runs on Kubernetes and is managed through Kubernetes interfaces.      If you are looking for documentation about how to use Metal¬≥, please check the user-guide.            Metal3 Component Overview:                     It is helpful to understand the high level architecture of of the Machine API Integration. Click on each step to learn more about that particular component.                   Machine controller:           Bare metal actuator                          The first component is the Bare Metal Actuator, which is an implementation of the Machine Actuator interface defined by the cluster-api project. This actuator reacts to changes to Machine objects and acts as a client of the BareMetalHost custom resources managed by the Bare Metal                           Bare metal operator:           With CRDs representing bare metal inventory with configuration needed by its bare metal management workers.                             The architecture also includes a new Bare Metal Operator, which includes the following:             A Controller for a new Custom Resource, BareMetalHost. This custom resource represents an inventory of known (configured or automatically discovered) bare metal hosts. When a Machine is created the Bare Metal Actuator will claim one of these hosts to be provisioned as a new Kubernetes node.             In response to BareMetalHost updates, the controller will perform bare metal host provisioning actions as necessary to reach the desired state.             The creation of the BareMetalHost inventory can be done in two ways:                           Manually via creating BareMetalHost objects.               Optionally, automatically created via a bare metal host discovery process.                 For more information about Operators, see the operator-sdk.                                         Bare metal management pods:                           The operator manages a set of tools for controlling the power on the host, monitoring the host status, and provisioning images to the host. These tools run inside the pod with the operator, and do not require any configuration by the user.                                       APIs:    Enroll nodes by creating BareMetalHost resources. This would either bemanually or done by a component doing node discovery and introspection.   See the documentation in thebaremetal-operator repository for details.     Use the machine API to allocate a machine.   See the documentation in thecluster-api-provider-metal3repository for details.     The new Machine is associated with an available BareMetalHost, which triggersthe provisioning of that host to join the cluster. This association is done bythe Actuator when it sets the MachineRef field on the BareMetalHost.  Design Documents: The design documents for Metal3 are all publicly available. Refer to the metal3-io/metal3-docs github repository for details. Around the Web: Conference Talks:  Metal¬≥: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStack‚Äôs Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hat‚Äôs MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  Metal¬≥ Blog posts  "
    }, {
    "id": 21,
    "url": "/blog/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;                   One cluster - multiple providers:         Friday, 8/07/2022        By Lennart Jern        Running on bare metal has both benefits and drawbacks. You can get the best performance possible out of the hardware, but it can also be quite expensive and maybe not necessary for all workloads. Perhaps a hybrid cluster could give you the best of both? Raw power for the workload. . .         Read More                    Metal3 Introduces Pivoting:         Wednesday, 5/05/2021        By Kashif Nizam Khan        Metal3 project has introduced pivoting in its CI workflow. The motivation for pivoting is to move all the objects from the ephemeral/management cluster to a target cluster. This blog post will briefly introduce the concept of pivoting and the impact it has on the overall CI workflow. For the rest. . .         Read More                    Introducing the Metal3 IP Address Manager:         Monday, 6/07/2020        By Ma√´l Kimmerlin        As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4 release, the Metal3 crew introduced a new project: its own IP Address Manager. This blog post will go through the motivations behind such a project, the features that it brings, its use in Metal3 and future work. What. . .         Read More                    Raw image streaming available in Metal3:         Sunday, 5/07/2020        By Ma√´l Kimmerlin        Metal3 supports multiple types of images for deployment, the most popular being QCOW2. We have recently added support for a feature of Ironic that improves deployments on constrained environments, raw image streaming. We‚Äôll first dive into how Ironic deploys the images on the target hosts, and how raw image streaming. . .         Read More                    Metal¬≥ development environment walkthrough part 2: Deploying a new bare metal cluster:         Thursday, 18/06/2020        By Himanshu Roy        Introduction This blog post describes how to deploy a bare metal cluster, a virtual one for simplicity, using Metal¬≥/metal3-dev-env. We will briefly discuss the steps involved in setting up the cluster as well as some of the customization available. If you want to know more about the architecture of Metal¬≥,. . .         Read More                                             1                            2                            3                            4                                                                                              Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move      "
    }, , {
    "id": 22,
    "url": "/privacy-statement.html",
    "title": "Privacy Statement",
    "author" : "",
    "tags" : "",
    "body": " -   Privacy Statement&lt;/section&gt;           Privacy Statement for the Metal¬≥ Project: As Metal3. io and most of the infrastructure of the Metal¬≥ Project are currently hosted by Red Hat Inc. , this site falls under the Red Hat Privacy Policy. All terms of that privacy policy apply to this site. Should we change our hosting in the future, this Privacy Policy will be updated. How to Contact Us: If you have any questions about any of these practices or Metal¬≥‚Äôs use of your personal information, please feel free to contact us or file an Issue in our GitHub repo. Metal¬≥ will work with you to resolve any concerns you may have about this Statement. Changes to this Privacy Statement: Metal¬≥ reserves the right to change this policy from time to time. If we do make changes, the revised Privacy Statement will be posted on this site. A notice will be posted on our blog and/or mailing lists whenever this privacy statement is changed in a material way. This Privacy Statement was last amended on September 25, 2019. "
    }, , , {
    "id": 23,
    "url": "/try-it.html",
    "title": "Try it: Getting started with Metal3.io",
    "author" : "",
    "tags" : "",
    "body": " -  1. Environment Setup     1. 1. Prerequisites   1. 2. Setup   1. 3. Tear Down   1. 4. Using Custom Image   1. 5. Setting environment variables    2. Working with Environment     2. 1. BareMetalHosts   2. 2. Provision Cluster and Machines   2. 3. Deprovision Cluster and Machines   2. 4. Running Custom Baremetal-Operator   2. 5. Running Custom Cluster API Provider Metal3         Tilt development environment          2. 6. Accessing Ironic API   1. Environment Setup:  info ‚ÄúNaming‚ÄùFor the v1alpha3 release, the Cluster API provider for Metal3 was renamed fromCluster API provider BareMetal (CAPBM) to Cluster API provider Metal3 (CAPM3). Hence,from v1alpha3 onwards it is Cluster API provider Metal3. 1. 1. Prerequisites:  System with CentOS 8 Stream or Ubuntu 20. 04 Bare metal preferred, as we will be creating VMs to emulate bare metal hosts Run as a user with passwordless sudo access Minimum resource requirements for the host machine: 4C CPUs, 16 GB RAM memory1. 2. Setup:  info ‚ÄúInformation‚ÄùIf you need detailed information regarding the process of creating a Metal¬≥ emulated environment using metal3-dev-env, it is worth taking a look at the blog post ‚ÄúA detailed walkthrough of the Metal¬≥ development environment‚Äù. This is a high-level architecture of the Metal¬≥-dev-env. Note that for an Ubuntu-based setup, either Kind or Minikube can be used to instantiate an ephemeral cluster, while for a CentOS-based setup, only Minikube is currently supported. The ephemeral cluster creation tool can be manipulated with the EPHEMERAL_CLUSTER environment variable.   The short version is: clone metal¬≥-dev-envand run $ makeThe Makefile runs a series of scripts, described here:    01_prepare_host. sh - Installs all needed packages.     02_configure_host. sh - Creates a set of VMs that will be managed as if theywere bare metal hosts. It also downloads some images needed for Ironic.     03_launch_mgmt_cluster. sh - Launches a management cluster using minikube or kindand runs the baremetal-operator on that cluster.     04_verify. sh - Runs a set of tests that verify that the deployment was completed successfully.  When the environment setup is completed, you should be able to see the BareMetalHost (bmh) objects in the Ready state. 1. 3. Tear Down: To tear down the environment, run $ make clean info ‚ÄúNote‚ÄùWhen redeploying metal¬≥-dev-env with a different release version of CAPM3, youmust set the FORCE_REPO_UPDATE variable in config_${user}. sh to true.  warning ‚ÄúWarning‚ÄùIf you see this error during the installation:  error: failed to connect to the hypervisor \error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied  You may need to log out then log in again, and run make clean and make again. 1. 4. Using Custom Image: Whether you want to run target cluster Nodes with your own image, you can override the three following variables: IMAGE_NAME,IMAGE_LOCATION, IMAGE_USERNAME. If the requested image with the name IMAGE_NAME does notexist in the IRONIC_IMAGE_DIR (/opt/metal3-dev-env/ironic/html/images) folder, then it will be automaticallydownloaded from the IMAGE_LOCATION value configured. 1. 5. Setting environment variables:  info ‚ÄúEnvironment variables‚ÄùMore information about the specific environment variables used to set up metal3-dev-env can be found here. To set environment variables persistently, export them from the configuration file used by metal¬≥-dev-env scripts: $ cp config_example. sh config_$(whoami). sh$ vim config_$(whoami). sh2. Working with Environment: 2. 1. BareMetalHosts: This environment creates a set of VMs to manage as if they were bare metalhosts. There are two different host OSs that the metal3-dev-env setup process is tested on.  Host VM/Server on CentOS, while the target can be Ubuntu or CentOS, Cirros, or FCOS.  Host VM/Server on Ubuntu, while the target can be Ubuntu or CentOS, Cirros, or FCOS. The way the k8s cluster is running in the above two scenarios is different. For CentOS minikube cluster is used as the source cluster, for Ubuntu, a kind cluster is being created. As such, when the host (where the make command was issued) OS is CentOS, there should be three libvirt VMs and one of them should be a minikube VM. In case the host OS is Ubuntu, the k8s source cluster is created by using kind, so in this case the minikube VM won‚Äôt be present. To configure what tool should be used for creating source k8s cluster the EPHEMERAL_CLUSTER environment variable is responsible. The EPHEMERAL_CLUSTER is configured to build minikube cluster by default on a CentOS host and kind cluster on a Ubuntu host. VMs can be listed using virsh cli tool. In case the EPHEMERAL_CLUSTER environment variable is set to kind the list ofrunning virtual machines will look like this: $ sudo virsh list Id  Name    State-------------------------- 1   node_0   running 2   node_1   runningIn case the EPHEMERAL_CLUSTER environment variable is set to minikube the list ofrunning virtual machines will look like this: $ sudo virsh list Id  Name    State-------------------------- 1  minikube  running 2  node_0   running 3  node_1   runningEach of the VMs (aside from the minikube management cluster VM) isrepresented by BareMetalHost objects in our management cluster. The yamldefinition file used to create these host objects is in bmhosts_crs. yaml. $ kubectl get baremetalhosts -n metal3 -o wideNAME   STATUS  STATE    CONSUMER  BMC                                             HARDWARE_PROFILE  ONLINE  ERROR  AGEnode-0  OK    available       ipmi://192. 168. 111. 1:6230                                  unknown      true       58mnode-1  OK    available       redfish+http://192. 168. 111. 1:8000/redfish/v1/Systems/492fcbab-4a79-40d7-8fea-a7835a05ef4a  unknown      true       58mYou can also look at the details of a host, including the hardware informationgathered by doing pre-deployment introspection. $ kubectl get baremetalhost -n metal3 -o yaml node-0apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: annotations:  kubectl. kubernetes. io/last-applied-configuration: |   { apiVersion : metal3. io/v1alpha1 , kind : BareMetalHost , metadata :{ annotations :{}, name : node-0 , namespace : metal3 }, spec :{ bmc :{ address : ipmi://192. 168. 111. 1:6230 , credentialsName : node-0-bmc-secret }, bootMACAddress : 00:ee:d0:b8:47:7d , bootMode : legacy , online :true}} creationTimestamp:  2021-07-12T11:04:10Z  finalizers: - baremetalhost. metal3. io generation: 1 name: node-0 namespace: metal3 resourceVersion:  3243  uid: 3bd8b945-a3e8-43b9-b899-2f869680d28cspec: automatedCleaningMode: metadata bmc:  address: ipmi://192. 168. 111. 1:6230  credentialsName: node-0-bmc-secret bootMACAddress: 00:ee:d0:b8:47:7d bootMode: legacy online: truestatus: errorCount: 0 errorMessage:    goodCredentials:  credentials:   name: node-0-bmc-secret   namespace: metal3  credentialsVersion:  1789  hardware:  cpu:   arch: x86_64   clockMegahertz: 2694   count: 2   flags:    - aes    - apic    # There are many more flags but they are not listed in this example.    model: Intel Xeon E3-12xx v2 (Ivy Bridge)  firmware:   bios:    date: 04/01/2014    vendor: SeaBIOS    version: 1. 13. 0-1ubuntu1. 1  hostname: node-0  nics:  - ip: 172. 22. 0. 20   mac: 00:ee:d0:b8:47:7d   model: 0x1af4 0x0001   name: enp1s0   pxe: true  - ip: fe80::1863:f385:feab:381c%enp1s0   mac: 00:ee:d0:b8:47:7d   model: 0x1af4 0x0001   name: enp1s0   pxe: true  - ip: 192. 168. 111. 20   mac: 00:ee:d0:b8:47:7f   model: 0x1af4 0x0001   name: enp2s0  - ip: fe80::521c:6a5b:f79:9a75%enp2s0   mac: 00:ee:d0:b8:47:7f   model: 0x1af4 0x0001   name: enp2s0  ramMebibytes: 4096  storage:  - hctl:  0:0:0:0    model: QEMU HARDDISK   name: /dev/sda   rotational: true   serialNumber: drive-scsi0-0-0-0   sizeBytes: 53687091200   type: HDD   vendor: QEMU  systemVendor:   manufacturer: QEMU   productName: Standard PC (Q35 + ICH9, 2009) hardwareProfile: unknown lastUpdated:  2021-07-12T11:08:53Z  operationHistory:  deprovision:   end: null   start: null  inspect:   end:  2021-07-12T11:08:23Z    start:  2021-07-12T11:04:55Z   provision:   end: null   start: null  register:   end:  2021-07-12T11:04:55Z    start:  2021-07-12T11:04:44Z  operationalStatus: OK poweredOn: true provisioning:  ID: 8effe29b-62fe-4fb6-9327-a3663550e99d  bootMode: legacy  image:   url:     rootDeviceHints:   deviceName: /dev/sda  state: ready triedCredentials:  credentials:   name: node-0-bmc-secret   namespace: metal3  credentialsVersion:  1789 2. 2. Provision Cluster and Machines: This section describes how to trigger the provisioning of a cluster and hosts viaMachine objects as part of the Cluster API integration. This uses Cluster APIv1beta1 andassumes that metal3-dev-env is deployed with the environment variableCAPM3_VERSION set to v1beta1. This is the default behaviour. The v1beta1 deployment can be done withUbuntu 20. 04 or Centos 8 Stream target host images. Please make sure to meetresource requirements for successful deployment: The following scripts can be used to provision a cluster, controlplane node and worker node. $ . /scripts/provision/cluster. sh$ . /scripts/provision/controlplane. sh$ . /scripts/provision/worker. shAt this point, the Machine actuator will respond and try to claim aBareMetalHost for this Metal3Machine. You can check the logs of the actuator. First, check the names of the pods running in the baremetal-operator-system namespace and the output should be something similarto this: $ kubectl -n baremetal-operator-system get podsNAME                          READY  STATUS  RESTARTS  AGEbaremetal-operator-controller-manager-5fd4fb6c8-c9prs  2/2   Running  0     71mIn order to get the logs of the actuator the logs of the baremetal-operator-controller-manager instance have to be queried withthe following command: $ kubectl logs -n baremetal-operator-system pod/baremetal-operator-controller-manager-5fd4fb6c8-c9prs -c manager. . . { level : info , ts :1642594214. 3598707, logger : controllers. BareMetalHost , msg : done , baremetalhost : metal3/node-1 ,  provisioningState : provisioning , requeue :true, after :10}. . . Keep in mind that the suffix hashes e. g. 5fd4fb6c8-c9prs are automatically generated and change in case of a differentdeployment. If you look at the yaml representation of the Metal3Machine object, you will see anew annotation that identifies which BareMetalHost was chosen to satisfy thisMetal3Machine request. First list the Metal3Machine objects present in the metal3 namespace: $ kubectl get metal3machines -n metal3NAME            PROVIDERID                   READY  CLUSTER  PHASEtest1-controlplane-jjd9l  metal3://d4848820-55fd-410a-b902-5b2122dd206c  true  test1test1-workers-bx4wp    metal3://ee337588-be96-4d5b-95b9-b7375969debd  true  test1Based on the name of the Metal3Machine objects you can check the yaml representation of the object andsee from its annotation which BareMetalHost was chosen. $ kubectl get metal3machine test1-workers-bx4wp -n metal3 -o yaml. . .  annotations:  metal3. io/BareMetalHost: metal3/node-1. . . You can also see in the list of BareMetalHosts that one of the hosts is nowprovisioned and associated with a Metal3Machines by looking at the CONSUMER output column of the following command: $ kubectl get baremetalhosts -n metal3NAME   STATE     CONSUMER          ONLINE  ERROR  AGEnode-0  provisioned  test1-controlplane-jjd9l  true       122mnode-1  provisioned  test1-workers-bx4wp    true       122mIt is also possible to check which Metal3Machine serves as the infrastructure for the ClusterAPI Machineobjects. First list the Machine objects: $ kubectl get machine -n metal3NAME           CLUSTER  NODENAME         PROVIDERID                   PHASE   AGE  VERSIONtest1-6d8cc5965f-wvzms  test1   test1-6d8cc5965f-wvzms  metal3://7f51f14b-7701-436a-85ba-7dbc7315b3cb  Running  53m  v1. 22. 3test1-nphjx       test1   test1-nphjx       metal3://14fbcd25-4d09-4aca-9628-a789ba3e175c  Running  55m  v1. 22. 3As a next step you can check what serves as the infrastructure backend for e. g. test1-6d8cc5965f-wvzms Machineobject: $ kubectl get machine test1-6d8cc5965f-wvzms -n metal3 -o yaml. . .  infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1beta1  kind: Metal3Machine  name: test1-workers-bx4wp  namespace: metal3  uid: 39362b32-ebb7-4117-9919-67510ceb177f. . . Based on the result of the query test1-6d8cc5965f-wvzms ClusterAPI Machine object is backed bytest1-workers-bx4wp Metal3Machine object. You should be able to ssh into your host once provisioning is completed. The default username for both CentOS &amp; Ubuntu images is metal3. For the IP address, you can either use the API endpoint IP of the target clusterwhich is - 192. 168. 111. 249 by default or use the predictable IP address of the firstmaster node - 192. 168. 111. 100. $ ssh metal3@192. 168. 111. 2492. 3. Deprovision Cluster and Machines: Deprovisioning of the target cluster is done just by deleting Cluster and Machine objects or by executing the de-provisioning scripts in reverse order than provisioning: $ . /scripts/deprovision/worker. sh$ . /scripts/deprovision/controlplane. sh$ . /scripts/deprovision/cluster. shNote that you can easily de-provision worker Nodes by decreasing the number of replicas in the MachineDeployment object created when executing the provision/worker. sh script: $ kubectl scale machinedeployment test1 -n metal3 --replicas=0 warning ‚ÄúWarning‚Äùcontrol-plane and cluster are very tied together. This means that you are not able to de-provision the control-plane of a cluster and then provision a new one within the same cluster. Therefore, in case you want to de-provision the control-plane you need to de-provision the cluster as well and provision both again. Below, it is shown how the de-provisioning can be executed in a more manual way by just deleting the proper Custom Resources (CR). The order of deletion is:  Machine objects of the workers Metal3Machine objects of the workers Machine objects of the control plane Metal3Machine objects of the control plane The cluster objectAn additional detail is that the Machine object test1-workers-bx4wp is controlled by the test1 MachineDeploymentthe object thus in order to avoid reprovisioning of the Machine object the MachineDeployment has to be deleted instead of the Machine object in the case of test1-workers-bx4wp. $ # By deleting the Machine or MachineDeployment object the related Metal3Machine object(s) should be deleted automatically. $ kubectl delete machinedeployment test1 -n metal3machinedeployment. cluster. x-k8s. io  test1  deleted$ # The  machinedeployment. cluster. x-k8s. io  test1  deleted  output will be visible almost instantly but that doesn't mean that the related Machine$ # object(s) has been deleted right away, after the deletion command is issued the Machine object(s) will enter a  Deleting  state and they could stay in that state for minutes$ # before they are fully deleted. $ kubectl delete machine test1-m77bn -n metal3machine. cluster. x-k8s. io  test1-m77bn  deleted$ # When a Machine object is deleted directly and not by deleting a MachineDeployment the  machine. cluster. x-k8s. io  test1-m77bn  deleted  will be only visible when the Machine and the$ # related Metal3Machine object has been fully removed from the cluster. The deletion process could take a few minutes thus the command line will be unresponsive (blocked) for the time being. $ kubectl delete cluster test1 -n metal3cluster. cluster. x-k8s. io  test1  deletedOnce the deletion has finished, you can see that the BareMetalHosts are offline and Cluster object is not present anymore $ kubectl get baremetalhosts -n metal3NAME   STATE    CONSUMER  ONLINE  ERROR  AGEnode-0  available       false      160mnode-1  available       false      160m$ kubectl get cluster -n metal3No resources found in metal3 namespace. 2. 4. Running Custom Baremetal-Operator: The baremetal-operator comes up running in the cluster by default, using animage built from the metal3-io/baremetal-operator repository. If you‚Äôd like to test changes to thebaremetal-operator, you can follow this process. First, you must scale down the deployment of the baremetal-operator runningin the cluster. $ kubectl scale deployment baremetal-operator-controller-manager -n baremetal-operator-system --replicas=0To be able to run baremetal-operator locally, you need to installoperator-sdk. After that, you can runthe baremetal-operator including any custom changes. $ cd ~/go/src/github. com/metal3-io/baremetal-operator$ make run2. 5. Running Custom Cluster API Provider Metal3: There are two Cluster API-related managers running in the cluster. Oneincludes a set of generic controllers, and the other includes a custom Machinecontroller for Metal3. Tilt development environment: Tilt setup can deploy CAPM3 in a local kind cluster. SinceTilt is applied in the metal3-dev-env deployment, you can make changes insidethe cluster-api-provider-metal3 folder and Tilt will deploy the changesautomatically. If you deployed CAPM3 separately and want to make changes to it, thenfollow CAPM3 instructions. This will save you fromhaving to build all of the images for CAPI, which can take a while. If thescope of your development will span both CAPM3 and CAPI, then follow theCAPI and CAPM3 instructions. 2. 6. Accessing Ironic API: Sometimes you may want to look directly at Ironic to debug something. The metal3-dev-env repository contains clouds. yaml file withconnection settings for Ironic. Metal3-dev-env will install the unified OpenStack and standaloneOpenStack Ironic command-line clients on the provisioning host aspart of setting up the cluster. Note that currently, you can use either a unified OpenStack clientor an Ironic client. In this example, we are using an Ironic client to interactwith the Ironic API. Please make sure to exportCONTAINER_RUNTIME environment variable before you executecommands. Example: [notstack@metal3 metal3-dev-env]$ export CONTAINER_RUNTIME=docker[notstack@metal3 metal3-dev-env]$ baremetal node list+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| UUID                 | Name     | Instance UUID            | Power State | Provisioning State | Maintenance |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| b423ee9c-66d8-48dd-bd6f-656b93140504 | metal3~node-1 | 7f51f14b-7701-436a-85ba-7dbc7315b3cb | power off  | available     | False    || 882533c5-2f14-49f6-aa44-517e1e404fd8 | metal3~node-0 | 14fbcd25-4d09-4aca-9628-a789ba3e175c | power off  | available     | False    |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+To view a particular node‚Äôs details, run the below command. Thelast_error, maintenance_reason, and provisioning_state fields areuseful for troubleshooting to find out why a node did not deploy. [notstack@metal3 metal3-dev-env]$ baremetal node show b423ee9c-66d8-48dd-bd6f-656b93140504+------------------------+------------------------------------------------------------+| Field         | Value                           |+------------------------+------------------------------------------------------------+| allocation_uuid    | None                            || automated_clean    | True                            || bios_interface     | redfish                          || boot_interface     | ipxe                            || chassis_uuid      | None                            || clean_step       | {}                             || conductor       | 172. 22. 0. 2                         || conductor_group    |                              || console_enabled    | False                           || console_interface   | no-console                         || created_at       | 2022-01-19T10:56:06+00:00                 || deploy_interface    | direct                           || deploy_step      | {}                             || description      | None                            || driver         | redfish                          || driver_info      | {u'deploy_kernel': u'http://172. 22. 0. 2:6180/images/ironic-python-agent. kernel', u'deploy_ramdisk': u'http://172. 22. 0. 2:6180/images/ironic-python-agent. initramfs', u'redfish_address': u'http://192. 168. 111. 1:8000', u'redfish_password': u'******', u'redfish_system_id': u'/redfish/v1/Systems/492fcbab-4a79-40d7-8fea-a7835a05ef4a', u'redfish_username': u'admin', u'force_persistent_boot_device': u'Default'} || driver_internal_info  | {u'last_power_state_change': u'2022-01-19T13:04:01. 981882', u'agent_version': u'8. 3. 1. dev2', u'agent_last_heartbeat': u'2022-01-19T13:03:51. 874842', u'clean_steps': None, u'agent_erase_devices_iterations': 1, u'agent_erase_devices_zeroize': True, u'agent_continue_if_secure_erase_failed': False, u'agent_continue_if_ata_erase_failed': False, u'agent_enable_nvme_secure_erase': True, u'disk_erasure_concurrency': 1, u'agent_erase_skip_read_only': False, u'hardware_manager_version': {u'generic_hardware_manager': u'1. 1'}, u'agent_cached_clean_steps_refreshed': u'2022-01-19 13:03:47. 558697', u'deploy_steps': None, u'agent_cached_deploy_steps_refreshed': u'2022-01-19 12:09:34. 731244'} || extra         | {}                             || fault         | None                            || inspect_interface   | inspector                         || inspection_finished_at | None                            || inspection_started_at | 2022-01-19T10:56:17+00:00                 || instance_info     | {u'capabilities': {}, u'image_source': u'http://172. 22. 0. 1/images/CENTOS_8_NODE_IMAGE_K8S_v1. 22. 3-raw. img', u'image_os_hash_algo': u'md5', u'image_os_hash_value': u'http://172. 22. 0. 1/images/CENTOS_8_NODE_IMAGE_K8S_v1. 22. 3-raw. img. md5sum', u'image_checksum': u'http://172. 22. 0. 1/images/CENTOS_8_NODE_IMAGE_K8S_v1. 22. 3-raw. img. md5sum', u'image_disk_format': u'raw'} || instance_uuid     | None                            || last_error       | None                            || lessee         | None                            || maintenance      | False                           || maintenance_reason   | None                            || management_interface  | redfish                          || name          | metal3~node-1                       || network_data      | {}                             || network_interface   | noop                            || owner         | None                            || power_interface    | redfish                          || power_state      | power off                         || properties       | {u'capabilities': u'cpu_vt:true,cpu_aes:true,cpu_hugepages:true,boot_mode:bios', u'vendor': u'Sushy Emulator', u'local_gb': u'50', u'cpus': u'2', u'cpu_arch': u'x86_64', u'memory_mb': u'4096', u'root_device': {u'name': u's== /dev/sda'}}                                                                                            || protected       | False                           || protected_reason    | None                            || provision_state    | available                         || provision_updated_at  | 2022-01-19T13:03:52+00:00                 || raid_config      | {}                             || raid_interface     | no-raid                          || rescue_interface    | no-rescue                         || reservation      | None                            || resource_class     | None                            || retired        | False                           || retired_reason     | None                            || storage_interface   | noop                            || target_power_state   | None                            || target_provision_state | None                            || target_raid_config   | {}                             || traits         | []                             || updated_at       | 2022-01-19T13:04:03+00:00                 || uuid          | b423ee9c-66d8-48dd-bd6f-656b93140504            || vendor_interface    | redfish                          |+-------------------------------------------------------------------------------------+"
    }, , , , {
    "id": 24,
    "url": "/blog/page2/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 25,
    "url": "/blog/page3/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 26,
    "url": "/blog/page4/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, , ];

var idx = lunr(function () {
    this.ref('id')
    this.field('title', { boost: 2 })
    this.field('body')
    this.field('author')
    this.field('url')
    this.field('tags', { boost: 2 })
    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results </p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function getQueryVariable(variable) {
  var query = window.location.search.substring(1);
  var vars = query.split('&');

  for (var i = 0; i < vars.length; i++) {
    var pair = vars[i].split('=');

    if (pair[0] === variable) {
      return decodeURIComponent(pair[1].replace(/\+/g, '%20'));
    }
  }
}


var searchTerm = getQueryVariable('query');
if (searchTerm) {
  lunr_search(searchTerm)
}

</script>
<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>
</main>
<footer class="mk-main-footer">
  <div>
    <div class="mk-cncf-footer">
      <p>We are a <a href="https://cncf.io/">Cloud Native Computing Foundation</a> sandbox project.</p>
      <p><img src="/assets/images/cncf-color.png"/></p>
      <p>Copyright 2023 The Metal¬≥ Contributors - <a href="/privacy-statement.html">Privacy Statement</a></p>
      <p>Copyright 2023 The Linux Foundation. All Rights Reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a> page.</p>
    </div>
    <div class="mk-icons-footer">
      <p>
        <a href="https://twitter.com/metal3_io" aria-label="Visit us on Twitter">
          <i class="fab fa-twitter fa-lg"></i>
        </a>
        <a href="https://kubernetes.slack.com/messages/CHD49TLE7" data-placement="top" title="Join our Slack channel">
          <i class="fab fa-slack fa-lg"></i>
        </a>
        <a href="https://github.com/metal3-io" aria-label="View our repo on GitHub">
          <i class="fab fa-github fa-lg"></i>
        </a>
        <a href="https://groups.google.com/g/metal3-dev" aria-label="Send us an email">
          <i class="fas fa-envelope fa-lg"></i>
        </a>
        <a href="https://www.youtube.com/channel/UC_xneeYbo-Dl4g-U78xW15g/videos" aria-label="See our YouTube channel">
          <i class="fab fa-youtube fa-lg"></i>
        </a>
      </p>
    </div>
  </div>
</footer>
</div><!--wrapper-->
<script>
var toggle = document.querySelector('#toggle');
var menu = document.querySelector('#main_nav');
var menuItems = document.querySelectorAll('#main_nav li a');

toggle.addEventListener('click', function(){
if (menu.classList.contains('is-active')) {
  this.setAttribute('aria-expanded', 'false');
  menu.classList.remove('is-active');
} else {
  menu.classList.add('is-active');
  this.setAttribute('aria-expanded', 'true');
  //menuItems[0].focus();
}
});
</script>
    <script src="/assets/js/copy.js"></script>
    <!-- This comes from DTM/DPAL and must be latest entry in body-->

    <script type="text/javascript">
        if (("undefined" !== typeof _satellite) && ("function" === typeof _satellite.pageBottom)) {
            _satellite.pageBottom();
        }
    </script>
</body>
</html>

